

[
  
  
    
    
  
    
    
  
    
    
      {
        "title": "Ben",
        "excerpt": "Hi, I’m Ben. I’m currently pursuing my interest in flight as a PhD student in aerospace engineering at the University of Colorado Boulder. I completed my bachelor’s degree in mechanical engineering at George Fox University, outside Portland, Oregon. My current research interests center around explainable decision making under uncertainty and the development of autonomous systems for safety onboard manned and unmanned aircraft. When I’m not studying or working on research, I like to get outside and bike, rock climb, or play some soccer.\n",
        "content": "Hi, I’m Ben. I’m currently pursuing my interest in flight as a PhD student in aerospace engineering at the University of Colorado Boulder. I completed my bachelor’s degree in mechanical engineering at George Fox University, outside Portland, Oregon. My current research interests center around explainable decision making under uncertainty and the development of autonomous systems for safety onboard manned and unmanned aircraft. When I’m not studying or working on research, I like to get outside and bike, rock climb, or play some soccer.\n",
        "url": "/people/ben/"
      },
    
      {
        "title": "Himanshu",
        "excerpt": "Hey! I’m Himanshu. I am a Ph.D student in the Aerospace Department at CU Boulder. Currently, my research interests include - sequential decision making in partially observable environments that have arbitrary uncertainties, autonomous navigation for vehicles and manipulators among dynamic obstacles, and designing continuous action space online POMDP solvers. Prior to this, I finished my Masters degree in Computer Science from CU Boulder and my bachelor’s degree in Computer Science from Indian Institute of Technology (IIT) Ropar. In my free time I like to explore the beautiful city of Boulder and its neighboring towns on my bike, play basketball, cook delicious Indian curries, read mystery novels, or just watch animes and TV shows!\n",
        "content": "Hey! I’m Himanshu. I am a Ph.D student in the Aerospace Department at CU Boulder. Currently, my research interests include - sequential decision making in partially observable environments that have arbitrary uncertainties, autonomous navigation for vehicles and manipulators among dynamic obstacles, and designing continuous action space online POMDP solvers. Prior to this, I finished my Masters degree in Computer Science from CU Boulder and my bachelor’s degree in Computer Science from Indian Institute of Technology (IIT) Ropar. In my free time I like to explore the beautiful city of Boulder and its neighboring towns on my bike, play basketball, cook delicious Indian curries, read mystery novels, or just watch animes and TV shows!\n",
        "url": "/people/himanshu/"
      },
    
      {
        "title": "Jackson",
        "excerpt": "Hey! My name is Jackson and I’m a Master’s student in the Electrical Engineering department at CU Boulder studying dynamics and controls. I recieved my bachelor’s in Electrical engineering from Washington State University. My current research involves using reinforcement learning to find causes of failure in systems, a method called Adaptive Stress Testing. In my free time I enjoy playing games such as chess and Magic: The Gathering.\n",
        "content": "Hey! My name is Jackson and I’m a Master’s student in the Electrical Engineering department at CU Boulder studying dynamics and controls. I recieved my bachelor’s in Electrical engineering from Washington State University. My current research involves using reinforcement learning to find causes of failure in systems, a method called Adaptive Stress Testing. In my free time I enjoy playing games such as chess and Magic: The Gathering.\n",
        "url": "/people/jackson/"
      },
    
      {
        "title": "John",
        "excerpt": "Hi! My name is John and I am a first year M.Sc. student in the Aerospace Engineering Department at CU Boulder. I received my Bachelor’s degree in Aerospace Engineering from CU Boulder in May 2021. My current research broadly deals with validating cyber-physical systems using a reinforcement learning framework known as Adaptive Stress Testing (AST). This work seeks to provide an understanding of where safety-critical systems can slip into a failure regime. Outside of research I enjoy reading and mountain biking!\n",
        "content": "Hi! My name is John and I am a first year M.Sc. student in the Aerospace Engineering Department at CU Boulder. I received my Bachelor’s degree in Aerospace Engineering from CU Boulder in May 2021. My current research broadly deals with validating cyber-physical systems using a reinforcement learning framework known as Adaptive Stress Testing (AST). This work seeks to provide an understanding of where safety-critical systems can slip into a failure regime. Outside of research I enjoy reading and mountain biking!\n",
        "url": "/people/john/"
      },
    
      {
        "title": "Juyeop",
        "excerpt": "Hola! I am Juyeop. I am a visiting scholar in the Department of Aerospace Engineering at CU Boulder. I am pursuing a Master’s degree in Aerospace Engineering at KAIST. I received my Bachelor’s degree in Mechanical Engineering from Seoul National University. My main research interest includes the safety of robots and autonomous vehicles with systemic uncertainty. I am also interested in the verification or maximizing robustness of learning-based systems. In my free time, I enjoy workouts and watching movies and dramas. ‘My mister’ is the most favorite drama ever!\n",
        "content": "Hola! I am Juyeop. I am a visiting scholar in the Department of Aerospace Engineering at CU Boulder. I am pursuing a Master’s degree in Aerospace Engineering at KAIST. I received my Bachelor’s degree in Mechanical Engineering from Seoul National University. My main research interest includes the safety of robots and autonomous vehicles with systemic uncertainty. I am also interested in the verification or maximizing robustness of learning-based systems. In my free time, I enjoy workouts and watching movies and dramas. ‘My mister’ is the most favorite drama ever!\n",
        "url": "/people/juyeop/"
      },
    
      {
        "title": "Lisa",
        "excerpt": "Hi, I’m Lisa. I am a PhD student in the Aerospace Engineering Department at CU Boulder. Currently, my research interests include human-autonomy teaming, risk management in partially observable environments with uncertainty, autonomous navigation to minimize risk, and optimizing  human trust of collaborative autonomous systems. Prior to this, I earned a masters degree in Computer Science from Georgia Tech. I’m co-advised by Allie Hayman in the Bioastronautics focus area and work on trust modeling and prediction.\n",
        "content": "Hi, I’m Lisa. I am a PhD student in the Aerospace Engineering Department at CU Boulder. Currently, my research interests include human-autonomy teaming, risk management in partially observable environments with uncertainty, autonomous navigation to minimize risk, and optimizing  human trust of collaborative autonomous systems. Prior to this, I earned a masters degree in Computer Science from Georgia Tech. I’m co-advised by Allie Hayman in the Bioastronautics focus area and work on trust modeling and prediction.\n",
        "url": "/people/lisa/"
      },
    
      {
        "title": "Mark",
        "excerpt": "Hi I’m Mark!  I’m currently a PhD student in aerospace engineering at CU Boulder.  I received my Bachelors in Aeronautical Engineering from the US Air Force Academy and my Master’s in the same from MIT.  My research will hopefully focus upon better understanding the critical attributes and consequences of explainability in human-autonomy teaming in high-criticality environments.  I am a test pilot in the Air Force with experience mostly in the F-35 and T-38, but have been lucky enough to fly over 20 different types of aircraft over 10 years of flying.  Outside of work, I love getting outdoors hiking, camping, mountain biking, skiing, or playing golf.\n",
        "content": "Hi I’m Mark!  I’m currently a PhD student in aerospace engineering at CU Boulder.  I received my Bachelors in Aeronautical Engineering from the US Air Force Academy and my Master’s in the same from MIT.  My research will hopefully focus upon better understanding the critical attributes and consequences of explainability in human-autonomy teaming in high-criticality environments.  I am a test pilot in the Air Force with experience mostly in the F-35 and T-38, but have been lucky enough to fly over 20 different types of aircraft over 10 years of flying.  Outside of work, I love getting outdoors hiking, camping, mountain biking, skiing, or playing golf.\n",
        "url": "/people/mark/"
      },
    
      {
        "title": "Mel",
        "excerpt": "I’m a third-year PhD student at Vanderbilt University, working with ADCL over the summer of 2025 to develop a Julia framework for working with fully continuous, differentiable partially observable multiagent scenarios. At my home lab - the Vanderbilt Mathematical Programming and Intelligent Robotics Lab (VAMPIR) - I build algorithms for imperfect information games in robotics, targeting partially competitive, partially cooperative applications. In the past / on the side, I’ve worked with UAVs for conservation, computer vision in surgical robotics, and machine learning for road maintenance monitoring.\n\nI completed my Bachelor’s degree in Computer Science at Yale University in 2022. In my spare time, I enjoy board games and writing, and I’m an amateur sewist.\n",
        "content": "I’m a third-year PhD student at Vanderbilt University, working with ADCL over the summer of 2025 to develop a Julia framework for working with fully continuous, differentiable partially observable multiagent scenarios. At my home lab - the Vanderbilt Mathematical Programming and Intelligent Robotics Lab (VAMPIR) - I build algorithms for imperfect information games in robotics, targeting partially competitive, partially cooperative applications. In the past / on the side, I’ve worked with UAVs for conservation, computer vision in surgical robotics, and machine learning for road maintenance monitoring.\n\nI completed my Bachelor’s degree in Computer Science at Yale University in 2022. In my spare time, I enjoy board games and writing, and I’m an amateur sewist.\n",
        "url": "/people/mel/"
      },
    
      {
        "title": "Michael",
        "excerpt": "I am a PhD student in Electrical Engineering and Computer Sciences - Control, Intelligent Systems, and Robotics (CIR) at UC Berkeley, grateful to be advised by Profs. Claire J. Tomlin and Zachary N. Sunberg. I am interested in developing autonomous systems and architectures that integrate learning, control, and planning to enable physical robots to operate intelligently, safely, and efficiently, with provable guarantees. Currently, my research approaches include:\n\n  Integrating computer vision, machine learning, path planning and control systems to solve realistic robotic problems with high dimensional observations, spanning robotic navigation and manipulation.\n  Devising provable and efficient online algorithms for MDPs and POMDPs with continuous and hybrid spaces using importance sampling and continuous multi-armed bandits.\n  Applying motion planning techniques to other domains including nuclear radiation detection with drones and ecological population dynamics control.\n\n",
        "content": "I am a PhD student in Electrical Engineering and Computer Sciences - Control, Intelligent Systems, and Robotics (CIR) at UC Berkeley, grateful to be advised by Profs. Claire J. Tomlin and Zachary N. Sunberg. I am interested in developing autonomous systems and architectures that integrate learning, control, and planning to enable physical robots to operate intelligently, safely, and efficiently, with provable guarantees. Currently, my research approaches include:\n\n  Integrating computer vision, machine learning, path planning and control systems to solve realistic robotic problems with high dimensional observations, spanning robotic navigation and manipulation.\n  Devising provable and efficient online algorithms for MDPs and POMDPs with continuous and hybrid spaces using importance sampling and continuous multi-armed bandits.\n  Applying motion planning techniques to other domains including nuclear radiation detection with drones and ecological population dynamics control.\n\n",
        "url": "/people/michael/"
      },
    
      {
        "title": "Ofer",
        "excerpt": "I am a new postdoc at ADCL, starting January 2024. I hold a Ph.D. in aerospace engineering sciences from CU Boulder. Prior to coming to CU Boulder, I worked in an R&amp;D aerospace industry for about 10 years as a research engineer and team leader.\nI hold a B.S. degree in aerospace engineering (2010, summa cum laude), and an M.S. degree in mechanical engineering (2015, cum laude), from the Technion – Israel Institute of Technology, Haifa, Israel.\nMy research interests include theory and algorithms for decentralized Bayesian reasoning and decision making under uncertainty in heterogeneous autonomous systems.\n",
        "content": "I am a new postdoc at ADCL, starting January 2024. I hold a Ph.D. in aerospace engineering sciences from CU Boulder. Prior to coming to CU Boulder, I worked in an R&amp;D aerospace industry for about 10 years as a research engineer and team leader.\nI hold a B.S. degree in aerospace engineering (2010, summa cum laude), and an M.S. degree in mechanical engineering (2015, cum laude), from the Technion – Israel Institute of Technology, Haifa, Israel.\nMy research interests include theory and algorithms for decentralized Bayesian reasoning and decision making under uncertainty in heterogeneous autonomous systems.\n",
        "url": "/people/ofer/"
      },
    
      {
        "title": "Qiheng",
        "excerpt": "Hey there! I’m Qi Heng, a PhD student at CU Boulder. My research interests include sequential decision making under uncertainty, formal synthesis for robotics, and the intersection of these fields. I am especially interested in understanding how we can develop time and safety-critical autonomous systems that perform reliably with behavioral guarantees in partially observable and unstructured environments. Previously, I was a research engineer at the Singapore-MIT Alliance for Research and Technology Centre, where I worked on developing algorithms for self-driving vehicles. I grew up in Singapore, and completed my undergraduate degree at the National University of Singapore.\n",
        "content": "Hey there! I’m Qi Heng, a PhD student at CU Boulder. My research interests include sequential decision making under uncertainty, formal synthesis for robotics, and the intersection of these fields. I am especially interested in understanding how we can develop time and safety-critical autonomous systems that perform reliably with behavioral guarantees in partially observable and unstructured environments. Previously, I was a research engineer at the Singapore-MIT Alliance for Research and Technology Centre, where I worked on developing algorithms for self-driving vehicles. I grew up in Singapore, and completed my undergraduate degree at the National University of Singapore.\n",
        "url": "/people/qiheng/"
      },
    
      {
        "title": "Sujeong",
        "excerpt": "Hi! I am Su-Jeong. I’m a visiting scholar in the Department of Aerospace Engineering at CU Boulder. I’m pursuing a PhD degree in Aerospace Engineering at KAIST. I received my Bachelor’s degree in Aerospace Engineering at Sejong University and Masters in Aerospace Engineering at KAIST in the Laboratory for information and Control System. My research is focused on decision making under uncertainty such as unknown dynamics, partially observable systems, and imperfect/incomplete information. I am currently interested in learning-based methods to deal with uncertainties under game theoretical formulations. In my free time I enjoy cooking delicious Korean foods, singing, and watching dramas.\n",
        "content": "Hi! I am Su-Jeong. I’m a visiting scholar in the Department of Aerospace Engineering at CU Boulder. I’m pursuing a PhD degree in Aerospace Engineering at KAIST. I received my Bachelor’s degree in Aerospace Engineering at Sejong University and Masters in Aerospace Engineering at KAIST in the Laboratory for information and Control System. My research is focused on decision making under uncertainty such as unknown dynamics, partially observable systems, and imperfect/incomplete information. I am currently interested in learning-based methods to deal with uncertainties under game theoretical formulations. In my free time I enjoy cooking delicious Korean foods, singing, and watching dramas.\n",
        "url": "/people/sujeong/"
      },
    
      {
        "title": "Tyler",
        "excerpt": "I’m a PhD student at CU Boulder and received my bachelor’s degree in aerospace engineering at Rutgers University New Brunswick, New Jersey. My research interests lie at the intersection of decision making under uncertainty and game theory. I began my research by investigating POMDP solution methods for finding cost-effective coronavirus testing strategies. Since then, I’ve moved to applying counterfactual regret methods to custody maintenance of adversarial satellites in the space domain awareness field. I’m currently looking into bridging the gap between the deterministic/continuous strategies offered by solutions to differential games and the stochastic/sequential strategies offered by solutions to imperfect information extensive form games.\n",
        "content": "I’m a PhD student at CU Boulder and received my bachelor’s degree in aerospace engineering at Rutgers University New Brunswick, New Jersey. My research interests lie at the intersection of decision making under uncertainty and game theory. I began my research by investigating POMDP solution methods for finding cost-effective coronavirus testing strategies. Since then, I’ve moved to applying counterfactual regret methods to custody maintenance of adversarial satellites in the space domain awareness field. I’m currently looking into bridging the gap between the deterministic/continuous strategies offered by solutions to differential games and the stochastic/sequential strategies offered by solutions to imperfect information extensive form games.\n",
        "url": "/people/tyler/"
      },
    
      {
        "title": "Will",
        "excerpt": "Hello, I’m Will. I am a Master’s student in the Aerospace Engineering Sciences department at CU Boulder. My current research focuses on the development and implementation of online POMDP planners on real-world hardware, using an autonomous ground robot for experimentation. This work aims to accelerate the adoption of these highly-capable decision-making algorithms in aerospace and robotics applications. I completed undergrad at the University of Washington in Seattle, where I studied aerospace engineering and worked on CubeSats. In my free time, I enjoy hiking, photography, and visiting museums.\n",
        "content": "Hello, I’m Will. I am a Master’s student in the Aerospace Engineering Sciences department at CU Boulder. My current research focuses on the development and implementation of online POMDP planners on real-world hardware, using an autonomous ground robot for experimentation. This work aims to accelerate the adoption of these highly-capable decision-making algorithms in aerospace and robotics applications. I completed undergrad at the University of Washington in Seattle, where I studied aerospace engineering and worked on CubeSats. In my free time, I enjoy hiking, photography, and visiting museums.\n",
        "url": "/people/will/"
      },
    
      {
        "title": "Zachary",
        "excerpt": "I’m an Assistant Professor in the Ann and H.J. Smead Aerospace Engineering Sciences Department. I earned Bachelors and Masters degrees in Aerospace Engineering from Texas A&amp;M University and a PhD in Aeronautics and Astronautics at Stanford University in the Stanford Intelligent Systems Lab. Before joining the University of Colorado faculty, I was a postdoctoral scholar at the University of California, Berkeley in the Hybrid Systems Lab. My research is focused on decision making under uncertainty to enable safe and efficient autonomous vehicle operation. My recent research has focused on developing online algorithms for approximately solving partially observable Markov decision processes (POMDPs) and games with continuous or hybrid state, action, and observation spaces. In my free time, I enjoy skiing, trail running (really more like trail slogging), volunteering at my church, and following long Wikipedia rabbit trails.\n",
        "content": "I’m an Assistant Professor in the Ann and H.J. Smead Aerospace Engineering Sciences Department. I earned Bachelors and Masters degrees in Aerospace Engineering from Texas A&amp;M University and a PhD in Aeronautics and Astronautics at Stanford University in the Stanford Intelligent Systems Lab. Before joining the University of Colorado faculty, I was a postdoctoral scholar at the University of California, Berkeley in the Hybrid Systems Lab. My research is focused on decision making under uncertainty to enable safe and efficient autonomous vehicle operation. My recent research has focused on developing online algorithms for approximately solving partially observable Markov decision processes (POMDPs) and games with continuous or hybrid state, action, and observation spaces. In my free time, I enjoy skiing, trail running (really more like trail slogging), volunteering at my church, and following long Wikipedia rabbit trails.\n",
        "url": "/people/zachary/"
      },
    
      {
        "title": "Zaki",
        "excerpt": "Hi, I’m Zaki. I am a second-year Ph.D. student in the Department of Aerospace Engineering at the University of Colorado Boulder. I received a B.Sc. and M.Sc. in Aerospace Engineering also at CU Boulder. My current research revolves around planning and control under uncertainty. I have worked on contingency management of UAS under component failures using MPC and POMDP methods. I am currently working on adaptive and transparent human-in-the-loop planning for search and rescue. In my free time, I enjoy playing soccer and shredding the gnar.\n",
        "content": "Hi, I’m Zaki. I am a second-year Ph.D. student in the Department of Aerospace Engineering at the University of Colorado Boulder. I received a B.Sc. and M.Sc. in Aerospace Engineering also at CU Boulder. My current research revolves around planning and control under uncertainty. I have worked on contingency management of UAS under component failures using MPC and POMDP methods. I am currently working on adaptive and transparent human-in-the-loop planning for search and rescue. In my free time, I enjoy playing soccer and shredding the gnar.\n",
        "url": "/people/zaki/"
      },
    
  
    
    
  
    
    
  
    
    
  
  
  
  {
    "title": "Zero to Reality: AlphaGo, MuZero and the Road to Real-World AI",
    "excerpt": "\n",
    "content": "\n\nFigure 1: Evolution of MuZero and its successors from 2016–2024.\n\nFrom defeating world champions in Go to tackling more general applications, the journey from AlphaGo to MuZero and its variants is a fascinating one. This post offers a concise overview of how these methods evolved, what sets them apart, and the challenges they aim to solve. Along the way, we’ll look at AlphaGo Zero, AlphaZero, MuZero, and their successors — Sampled, Unplugged, Stochastic, and EfficientZero — that pushed AI from zero-sum board games into stochastic and uncertain “real-world” environments.\n\nAlphaGo - 2016 [1]\n\nGo has around $250^{150}$ possible moves. Iterating over all possibilities to find an optimal policy is unfeasible. Instead, algorithms use some flavor of tree-search-based methods with additional handcrafted heuristics. Nevertheless, the issue of how to efficiently explore the tree remains. The innovation behind AlphaGo is to combine MCTS [1] with policy and value networks to obtain a very efficient tree search. But how is that achieved?\n\nAlphaGo has a combination of policies and value networks that estimate the action distribution for a given state and the expected game outcome. Initially, a policy is trained in a supervised manner (with 30 million positions), $p_\\sigma(a|s)$, which tries to predict expert moves. A smaller policy, $\\pi_r(a|s)$, is also trained with the goal of providing much faster evaluations for rollout (2 $\\mu\\text{s}$ against 3ms according to the authors). Then, a new policy $\\pi_p$ is initialized with the same weights as $\\pi_\\sigma$, which is used for self-play with randomly selected previous versions of the policy. $\\pi_p$ ended up winning 80\\% of the games against $\\pi_\\sigma$. The gradient used to train $\\pi_p$ follows:\n\n\n  \n    \n      [\\Delta p \\propto \\frac{\\partial \\pi_p(a\n      s)}{\\partial p}z]\n    \n  \n\n\nWith the policies networks trained, the value network, $v_\\theta(s)$, was obtained by predicting the outcome of the games, $z$, played by $\\pi_p$. To prevent overfitting, 30M positions were sampled from different games played with self-play. The gradient used was:\n\n[\\Delta \\theta \\propto \\frac{\\partial v_\\theta(s)}{\\partial \\theta}(z-v_\\theta(s))]\n\n\n\nFigure 2. AlphaGo training pipeline. Figure 1.a from [1].\n\nThen, the trained policies were combined with MCTS. Starting from the root node, each simulation will select actions based on\n\n[\\begin{split}\na_t &amp;= \\text{argmax}_a(Q(s_t,a) + u(s_t,a)) \nu(s,a) &amp; \\propto \\frac{P(s,a)}{1+N(s,a)}\n\\end{split}]\n\nwhere $P(s,a)=p_\\sigma(a|s)$. (instead of $\\pi_p$). When a leaf node is reached, $s_L$, the value network is called to provide an estimate of the outcome and the faster $\\pi_r$ is used to perform a rollout with outcome $z_L$. The value of the state is then obtained by combining both based on a parameter $\\lambda$:\n\n[V(s_L) = (1-\\lambda)v_\\theta(S_l)+\\lambda z_L]\n\nAfter reaching the leaf state, the action value estimation and the visit count are updated with:\n\n[\\begin{split}\nN(s,a) &amp; = \\sum_{i=1}^n 1(s,a,i) \nQ(s,a) &amp; = \\frac{1}{N(s,a)}\\sum_{i=1}^n 1(s,a,i)V(s_L^i)\n\\end{split}]\n\nThe action with the highest count in the root node is used in the game.\n\nGo’s invariance under reflection and rotation was explored during the networks’ training and MCTS. For example, the value of a given state is computed from the average of that state after undergoing all eight possible positions. During MCTS, one of the eight possibilities for a given state was sampled and used with $v_\\theta$ and $\\pi_\\sigma$ in the leaf node.\n\nThe network architecture for the policy function contains an input layer ($19\\times 19\\times 48$) followed by 12 hidden layers, which pad the previous hidden layer output into a $21\\times 21$ image with 192 filters and apply a ReLU. Then, a softmax function is applied to one last convolutional layer, outputting the action probabilities. The value network is similar, with an additional hidden layer, and two last fully-connected layers, the first with 256 with ReLU units and the second a linear layer with a tanh function that outputs the value for the given state.\n\nAlphaGo was tested against other Go programs, winning 99.8\\% of the matches. More interestingly, a distributed version of AlphaGo (which used more computing power) competed with Fan Hui, winning 5-0! After defeating Fan, the next challenge was going against Lee Sedol. To do so, an “intermediate” version between AlphaGo and AlphaGo Zero was created, referred to as AlphaGo Lee. AlphaGo Lee’s value function was obtained from AlphaGo self-play in an iterative process. Larger value and policy networks were also used, with 12 convolutional layers of 256 planes. During the game, many plays were remarkable (move 37, for example); a full documentary of AlphaGo, including the match with Lee is available on YouTube.\n\n\n\n\n\nFigure 3. AlphaGo performance against other algorithms. Figure 4.a from [1].\n\nA notable comparison made by the authors was studying AlphaGo’s performance when setting $\\lambda=0$ (only uses $v_\\theta$ on leaf nodes) or $\\lambda=1$ (only uses rollout with $\\pi_r$). The best result was obtained by setting $\\lambda=0.5$. This brings one of the questions addressed by its more powerful version, AlphaGo Zero: Can rollouts be removed?\n\nAlphaGo Zero - 2017 [3]\n\nGiven AlphaGo’s performance, the next question was: could an even more general algorithm learn entirely through self-play? AlphaGo Zero eliminates the initial supervised learning on expert data, learning from scratch (starting tabula rasa, as described by the authors) and without handcrafted features. The main changes compared to AlphaGo (and AlphaGo Lee) are:\n\n\n  No experience generated by experts to initialize training;\n  No handcrafted extra features as input, only the position of the pieces;\n  Single network that predicts both the value function and policy, $(\\mathbf{p}, v)=f_\\theta$;\n  No rollouts in the tree search, using only the value function returned by $f_\\theta$;\n\n\nStill, AlphaGo Zero has perfect knowledge of the game rules, which prevents illegal moves in the MCTS. Go’s invariance is also explored to improve the tree search.\n\nThe $f_\\theta$ network contains 20 residual blocks, each containing convolution layers, batch normalization, ReLU activations, and skipped connections. The output is connected to two heads, with the policy head having a convolution and fully-connected layers, and the value head having a convolution and two fully-connected layers. The loss function is:\n\n\n  \n    \n      [l=(z-v)^2-\\pi\\text{log}\\mathbf{p}+c\n       \n      \\theta\n       \n      ^2]\n    \n  \n\n\nwhere $c||\\theta||^2$ is a regularization term.\n\nSurprisingly, even without expert data, it surpassed AlphaGo Lee after 36 hours of training (which was trained for weeks). After 72 hours, it won all 100 games against AlphaGo Lee using a fraction of the compute cost (4 TPUs vs 48 TPUs).\n\n\n\n\n\nFigure 4. AlphaGo Zero performance comparison. Figure 3.a from [3].\n\nAlphaZero - 2018 [4]\n\nAlphaZero steps towards of a more general algorithm by being capable of playing not only Go but also Chess and Shogi. As indicated by the authors, the main differences with respect to the previous algorithms are:\n\n  Does not exploit the symmetry in Go to augment training data or during the tree search;\n  Uses the latest policy for self-play instead of the previous best.\n\n\nThe same network architecture and hyperparameters from AlphaGo Zero are used. Although being more general, AlphaZero achieved a similar performance to AlphaGo Zero. Interestingly, it surpassed Stockfish (2016 TCEC world champion program) after 4 hours of training in Chess and Elmo (2017 CSA world champion program) after 2 hours in Shogi.\n\n\n\nFigure 5. AlphaZero performance in Chess, Shogi, and Go. Figure 1 from [4].\n\nMuZero - 2020 [5]\n\nDespite the advances presented in AlphaZero, which requires no supervised training or handcrafted encodings, a simulator is still needed to perform the MCTS. When selecting an action $a$, the simulator is called to return the next state, $s’$. MuZero came to change this requirement by having an internal model of the environment, which it also learns during training. This brings the interesting question: what matters for learning in the environment? Should the model focus on specific features or represent the original environment as closely as possible?\n\nTo address that, MuZero plans on latent spaces instead of the history of observations. To do so, it has two new functions, the representation function $s^0=h_\\theta(01, \\cdots,o_t)$, and the dynamics function $r, s’ = f\\theta(s, a)$. Both are represented by DNNs.\n\nThe representation function maps the observation to the state. Still, there is no requirement on how well you can reverse the operation and reconstruct the original observation from the latent state. This means that only the most critical features for learning are extracted. Notably, once the root node state is obtained using $h_\\theta$, all the tree search is conducted in the latent state. The internal model of the world is obtained by the dynamics function, which outputs an estimated reward $r$ and next state $s$ from the current state and action, such that $r, s’ = f_\\theta(s, a)$. Therefore, MuZero has a total of three functions:\n\n\n  Representation function: $s^0 = h_θ(o^0)$ — encodes the observation history into a compact latent state.\n  Dynamics function: $(s’, r) = g_θ(s, a)$ — predicts the next latent state and immediate reward given a state and action.\n  Prediction function: $(p^k, V) = f_θ(s^k)$ - produces a policy distribution and scalar value from a latent state.\n\n\nThe total loss function used is\n\n\n  \n    \n      [l_\\text{MuZero} = \\sum_{k=0}^Kl_P(\\pi_{t+k}, p_t^k) + \\sum_{k=0}^K l^v(z_{t+k}, v_t^k) + \\sum_{k=1}^K l^r(u_{t+k}, r_t^k) + c\n       \n      \\theta\n       \n      ^2]\n    \n  \n\n\nwhere $\\pi$ is the action distribution at the root node returned from the MCTS, $z$ is the game’s output (win, lose, or draw for zero-sum games), and $u$ the reward at each step. Note that neither the state returned by the representation nor the encoder appears explicitly in the loss.\n\n\n\nFigure X. Illustration of MuZero tree search (a), interaction with the environment (b), and training with unrolled experience (c). Figure 1 from [5].\n\nThe results obtained by MuZero are similar to those from AlphaZero in Chess and Shogi, and slightly better in Go. The impressive part of the results is that the algorithm is capable of such high performance without having a simulator available for the tree search, using its learned model instead.\n\nAn interesting aspect of having the dynamics function returning $r$ is that MuZero can be applied to environments with intermediate rewards, not only at the end. This significantly extends MuZero’s applicability, moving from only zero-sum games (such as Chess, Go, and Shogi) to more general environments, such as Atari Games! The authors report that MuZero achieved SOTA in 57 games.\n\n\n\nFigure 6. MuZero performance in Chess, Shogi, Go, and Atari. Comparison against AlphaZero in zero-sum games and R2D2 in Atari. Figure 2 from [5].\n\nDespite high performance, one caveat is the high computation cost for training. Board games used 16 third-generation TPUs for training and 1,000 for self-play, while 8 TPUs were used for training and 32 for self-play in the Atari case. The authors addressed this limitation by introducing the Reanalyze technique, which improves the algorithm’s sample efficiency. Reanalyze works by re-running MCTS on older data to obtain new target policies and values for the loss function.\n\nIn short, MuZero advanced by not requiring a simulator while preserving performance and being applicable not only to zero-sum games. Still, some limitations remained since it only considers deterministic environments, is computationally expensive, and still doesn’t achieve zero-shot generalization as noted by the authors. As an application, a MuZero variant was used for YouTube video compression.\n\nSampled MuZero - 2021 [6]\n\nMuZero achieved SOTA without a simulator but was limited to discrete-action settings. Sampled MuZero provides an important generalization, being applied to cases with large action spaces and continuous actions. Interestingly, this extension is achieved with minimal changes. More specifically, at each tree node, $K$ actions are sampled from a distribution $\\beta$ during expansion, and search is conducted among those sampled actions. The equation used to select actions within the tree is\n\n[a = \\text{argmax}_a\\left(Q(s,a) + c(s)\\sigma\\frac{\\sqrt{\\sum_b N(s,b)}}{1+N(s,a)}\\right)]\n\nwhere $\\sigma=\\pi(s,a)$ in MuZero. Sampled MuZero uses $\\sigma=\\frac{\\hat{\\beta}}{\\beta}\\pi$ with $\\hat{\\beta}(a|s)=\\frac{1}{K}\\sum_i \\delta (a-a_i)$. At the node, actions are sampled from $\\beta=\\pi^{1\\tau}$ where $\\tau$ is a temperature parameter.\n\nSampled MuZero was tested in Go when varying $K={15, 25, 50, 100}$, with the case $K=100$ achieving a similar performance to MuZero (which can sample all 362 available actions). A similar result was obtained when testing it with Ms. Pacman, where good results were obtained with $K=3$ ($|\\mathcal{A}|=18$).\n\n\n\nFigure 7. Sampled MuZero performance in Go (left) and Ms. Pacman (right) against MuZero (all actions) with different $K$. Figure 2 from [6].\n\nResults were also presented for the case with a continuous action space in MuJoCo-based tasks. The algorithm networks were modified, replacing the convolutional layers with fully-connected layers, and Gaussian distributions represented the continuous actions. Interestingly, the case where the raw pixels were provided instead of the states was also tested.\n\nMuZero Unplugged - 2021 [7]\n\nMuZero Unplugged improved the Reanalyze technique introduced in MuZero, demonstrating that learning is possible without interacting with the environment (offline RL).\n\nReanalyze uses already generated trajectories to update the networks. A given sampled state $s_t$ has as associated action $a_t$ that was played, a reward $u_t$, the next state $s_{t+1}$, and the state value $z_t$. MuZero’s MCTS can be applied to $s_t$, resulting in a $\\pi_{\\text{MCTS}}$ being returned, the estimate of the value $v_t$, and the immediate reward $r_t$. Then, the same loss used for MuZero can be used to approximate those results. The main difference is that the action returned by the tree search is not used in the environment, as the trajectory was already generated.\n\nThe fraction of Reanalyze compared to environment interactions was named the Reanalyze fraction. Some uses of the Reanalyze technique are:\n\n\n  Increase efficiency by reanalyzing data sampled from the previous $N$ games (a prioritized replay buffer can be used instead, exploiting “good episodes”);\n  Offline RL;\n  Bootstrap from demonstrations;\n\n\nNo changes are made to MuZero other than the training pipeline. Interestingly, no off-policy corrections were made to the algorithms. Tested in Atari games, MuZero outperformed the DQN, IQN, and BCQ baselines. The authors also demonstrate offline RL with continuous action spaces using Sampled MuZero. The original action selected in the data from Reanalyze is added to the root node as one of the options to be explored, preventing cases where the algorithm would not sample the actions that were played. MuZero Unplugged’s performance is compared to several baselines - D4PG, BRAC, RABM, and behavioral cloning with MuZero - and achieved the best performance in most cases. MuZero Unplugged highlights how one single algorithm can achieve SOTA with both offline and online RL with continuous and discrete actions.\n\nStochastic MuZero - 2022 [8]\n\nOne of the main limitations remaining in MuZero is its applicability only to deterministic environments. To address this issue, the concept of afterstates is used, which can be understood as an intermediate state $as_t$ after applying action $a$ at state $s_t$ and before the state stochastically transitioning to other states with probability $T(s_{t+1}|as_t)=T(s_{t+1}|a,s_t)$. In doing so, Stochastic MuZero has five functions instead of the tree in regular MuZero:\n\n\n  Representation function: $s^0 = h_θ(o^0)$;\n  Dynamics function: $(s’, r) = g_θ(as, c)$ — predicts the next latent state and immediate reward given an afterstate state and chance;\n  Prediction function: $(p^k, V) = f_θ(s^k)$;\n  Afterstate dynamics function: $as_t=\\phi(s_t,a)$ - outputs the afterstate given a state and action;\n  Afterstate prediction function: $(\\sigma_t, Q_t)=\\psi(as_t)$ - outputs a distribution for the chance outcomes $c$.\n\n\nThe loss function is similar to the one used for MuZero with additional terms for the chance and $Q$ predictions:\n\n\n  \n    \n      [l_{\\text{SMuZero}} = l_{\\text{MuZero}} + sum_{k=0}^{K-1} l^Q(z,Q) + \\sum_{k=0}^{K-1}(c,\\sigma) + \\beta \\sum_{k=0}^{K-1}\n       \n      c-c^e\n       \n      ^2]\n    \n  \n\n\nwhere $c^e$ is the output of the variational autoencoder that is used for the chance prediction.\n\nThe MCTS is also modified to account for chance and decision nodes. Decision nodes were already present in previous versions, where the agent selects an action. Chance nodes are intermediate nodes between decision nodes (Figure 8).\n\n\n\nFigure 8. Stochastic MuZero MCTS (left) and training (right). Figure 1 from [8].\n\nThe algorithm’s performance is tested in the 2048 and Backgammon games. Interestingly, Stochastic MuZero’s performance matches AlphaZero’s (which uses a simulator for the tree search). It is also noteworthy that its performance matches standard MuZero in Go with additional generalization.\n\n\n\nFigure 9. Stochastic MuZero against regular MuZero, AlphaZero with perfect information, and Jaskowski algorithm. Figure 2 from [8].\n\nEfficientZero - 2021 / 2024 [9]\n\nEfficientZero aims to make MuZero more sample-efficient by addressing three points that hurt sample efficiency.\n\nIn MuZero Reanalyze, $z$ is computed using a trajectory already played and not representative of the current algorithm. To mitigate this issue, the authors suggest applying the MCTS to the last state of the Reanalyze unrolling to obtain a better estimate of that value. In practice, $z_t=\\sum_i^{l-1}\\gamma^i u_{t+i} + \\gamma^l v_{t+l}^{\\text{MCTS}}$ instead of $z_t=\\sum_i^{k-1}\\gamma^i u_{t+i} + \\gamma^k v_{t+k}$ where $v^{\\text{MCTS}}$ is returned by the new tree search and $l$ can change depending on how old the data is.\n\nMuZero doesn’t explicitly account for the prediction of the next state $s’$ in the loss, which can lead to mismatches. The authors add an extra loss term to help match the predicted state $\\hat{s}{t+1}$ (obtained by applying action $a$ at state $s_t$, which is obtained from the history $o_t$) to $s{t+1}$ (obtained directly from history $o_{t+1}$).\n\nThe estimation of the action values, $Q(s_t,a)$ can suffer from compounding errors. Instead of using the predicted rewards, $r_t$, such that $Q(s_t,a)=\\sum_{i=0}^{k-1}\\gamma^i r_{t+i} + \\gamma^k v_{t+k}$, a value prefix $G=m(s_t, \\hat{s}{t+1}, \\cdots, \\hat{s}{t+k-1})$, returned by a trained LSTM $m$, such that $Q(s_t,a)=G + \\gamma^k v_{t+k}$.\n\nAmong their results, the authors demonstrate that EfficientZero achieves super-human performance in the Atari 100k benchmark with only 2 hours of real-time gameplay, also surpassing other baseline algorithms.\n\nA newer version, EfficientZero V2 [10], applies extra modifications to its predecessor and accommodates continuous domains. The method also achieved SOTA with 2 hours of real-time gameplay. The authors made the code available on GitHub for the first and second versions.\n\nReferences:\n[1] M. J. Kochenderfer, T. A. Wheeler, and K. H. Wray, Algorithms for decision making. Cambridge, Massachusetts: The MIT Press, 2022.\n\n[2] D. Silver et al., “Mastering the game of Go with deep neural networks and tree search,” Nature, vol. 529, no. 7587, pp. 484–489, Jan. 2016, doi: 10.1038/nature16961.\n\n[3] D. Silver et al., “Mastering the game of Go without human knowledge,” Nature, vol. 550, no. 7676, pp. 354–359, Oct. 2017, doi: 10.1038/nature24270.\n\n[4] D. Silver et al., “A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play,” Science, vol. 362, no. 6419, pp. 1140–1144, 2018, doi: 10.1126/science.aar6404.\n\n[5] J. Schrittwieser et al., “Mastering Atari, Go, chess and shogi by planning with a learned model,” Nature, vol. 588, no. 7839, pp. 604–609, Dec. 2020, doi: 10.1038/s41586-020-03051-4.\n\n[6] T. Hubert, J. Schrittwieser, I. Antonoglou, M. Barekatain, S. Schmitt, and D. Silver, “Learning and Planning in Complex Action Spaces,” in Proceedings of the 38th International Conference on Machine Learning, M. Meila and T. Zhang, Eds., in Proceedings of Machine Learning Research, vol. 139. PMLR, July 2021, pp. 4476–4486.\n\n[7] J. Schrittwieser, T. Hubert, A. Mandhane, M. Barekatain, I. Antonoglou, and D. Silver, “Online and offline reinforcement learning by planning with a learned model,” in Proceedings of the 35th International Conference on Neural Information Processing Systems, in NIPS ‘21. Red Hook, NY, USA: Curran Associates Inc., 2021.\n\n[8] I. Antonoglou, J. Schrittwieser, S. Ozair, T. K. Hubert, and D. Silver, “Planning in Stochastic Environments with a Learned Model,” in International Conference on Learning Representations, 2022.\n\n[9] W. Ye, S. Liu, T. Kurutach, P. Abbeel, and Y. Gao, “Mastering Atari Games with Limited Data,” in Advances in Neural Information Processing Systems, M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. W. Vaughan, Eds., Curran Associates, Inc., 2021, pp. 25476–25488.\n\n[10] S. Wang, S. Liu, W. Ye, J. You, and Y. Gao, “EfficientZero V2: mastering discrete and continuous control with limited data,” in Proceedings of the 41st International Conference on Machine Learning, in ICML’24. Vienna, Austria: JMLR.org, 2024.\n",
    "url": "/posts/dmupp-2025/alphago-to-muzero/"
  },
  
  {
    "title": "Blog",
    "excerpt": "\n",
    "content": "This page contains blog posts from students and researchers in the ADCL and DMU++ course.\n\nDMU++ 2025 Student Posts\n\n\n  Communication Lessons from Combat Aviation for Unmanned Wingmen - Mark Boyer\n  Zero to Reality: AlphaGo, MuZero and the Road to Real-World AI - Lorenzzo Quevedo Mantovani\n  Generative Models hate this one trick to mitigate Observation Complexity - Gabriel Agostine\n  The Future of Safe RLHF (Reinforcement Learning from Human Feedback) - Lisa Ventura\n  Diffusion Models: A New Playground for Reinforcement Learning - Anaïs Cheval\n  When Will Robots Do My Chores? Progress and Promises in Bridging Simulation and Reality - Xavier O’keefe\n  Robust Trajectory RL - Daniel Huterer Prats\n  What are the various ways of finding a lost child in a music festival efficiently? - Kelvin Aladum\n  Oh, You Blockhead: Why Generalist Transformers Fail at Learning World Models - Ryan Draves\n  No Confusion: Discrete Diffusion - Sean Campbell\n  Some Background Diffusion Math - Sean Campbell\n  Bridging Language and Action: How Vision-Language-Action Models and Reinforcement Learning Enable Intelligent Robotic Decision Making - Lorin Achey\n  Soft Actor-Critic - Yumeka Nagano\n  Robust MDPs - Chris Guthrie\n\n",
    "url": "/blog/"
  },
  
  {
    "title": "CU Boulder AI and Robotics Classes",
    "excerpt": "\n",
    "content": "This is a collaborative list of courses related to AI and Robotics at CU Boulder. Please contribute! \nTo add more classes, re-organize, or eliminate courses that are not offered any more, edit this file: https://github.com/CU-ADCL/CU-ADCL.github.io/blob/main/posts/classes.md\n\nArtificial Intelligence\n\n\n  \n    \n      Department\n      Course Number\n      Course Title\n      Notes\n    \n  \n  \n    \n      AERO\n      5264\n      Decision Making under Uncertainty\n       \n    \n    \n      CSCI\n      5135\n      Computer-Aided Verification\n      Same as ECEN 5139\n    \n    \n      CSCI\n      5322\n      Algorithmic Foundations of Human-Robot Interaction\n       \n    \n    \n      CSCI\n      5352\n      Network Analysis and Modeling\n       \n    \n    \n      CSCI\n      5353\n      Datacenter Scale Computing\n      Cloud computing and deployment; useful for ML at scale\n    \n    \n      CSCI\n      5622\n      Machine Learning\n       \n    \n    \n      CSCI\n      5722\n      Computer Vision\n       \n    \n    \n      CSCI\n      5822\n      Probabilistic Models of Human and Machine Learning\n       \n    \n    \n      CSCI\n      5832\n      Natural Language Processing\n       \n    \n    \n      CSCI\n      5922\n      Neural Networks and Deep Learning\n       \n    \n    \n      CSCI\n      7000\n      Current Topics in Computer Science: Comp Models of Discourse\n       \n    \n    \n      CSCI\n      7000\n      Current Topics in Computer Science: Robotic Manipulation\n       \n    \n    \n      CSCI\n      7000-001\n      Deep Reinforcement Learning for Robotics\n       \n    \n    \n      ECEN\n      5712\n      Machine Learning for Engineers\n       \n    \n  \n\n\nMath\n\n\n  \n    \n      Department\n      Course Number\n      Course Title\n      Notes\n    \n  \n  \n    \n      AERO\n      5307\n      Engineering Data Analysis Methods\n       \n    \n    \n      AERO\n      5327\n      Experimental Design and Statistical Methods\n       \n    \n    \n      APPM\n      5440\n      Applied Analysis 1\n       \n    \n    \n      APPM\n      5510\n      Data Assimilation in High-Dim Dynamical Systems\n       \n    \n    \n      APPM\n      5515\n      High-Dimensional Probability for Data Science\n       \n    \n    \n      APPM\n      5530\n      Stochastic Analysis for Finance\n       \n    \n    \n      APPM\n      5650\n      Randomized Algorithms\n      Same as STAT 5650\n    \n    \n      APPM\n      5720-018\n      Open Topics in Applied Mathematics: Random Graphs\n       \n    \n    \n      APPM\n      5720-036\n      Open Topics in Applied Mathematics: Applied Deep Learning\n       \n    \n    \n      APPM\n      6560\n      Measure-Theoretic Probability\n       \n    \n    \n      APPM\n      8500\n      Statistics, Optimization and Machine Learning Seminar\n       \n    \n    \n      CSCI\n      5434\n      Probability for Computer Science\n       \n    \n    \n      CSCI\n      5606\n      Principles of Numerical Computation\n       \n    \n    \n      MATH\n      5510\n      Introduction to Probability Theory\n       \n    \n    \n      MATH\n      5520\n      Introduction to Mathematical Statistics\n       \n    \n    \n      MATH\n      6310\n      Introduction to Real Analysis 1\n       \n    \n    \n      STAT\n      5000\n      Statistical Methods and Application I\n       \n    \n    \n      STAT\n      5010\n      Statistical Methods and Applications II\n       \n    \n    \n      STAT\n      5430\n      Spatial Statistics\n       \n    \n    \n      STAT\n      5600\n      Methods in Statistical Learning\n       \n    \n    \n      STAT\n      5610\n      Statistical Learning\n       \n    \n  \n\n\nRobotics Estimation &amp; Controls\n\n\n  \n    \n      Department\n      Course Number\n      Course Title\n      Notes\n    \n  \n  \n    \n      AERO\n      5014\n      Linear Control Systems\n       \n    \n    \n      AERO\n      5044\n      Statistical Estimation for Dynamical Systems\n       \n    \n    \n      AERO\n      5067\n      Microavionics – Introduction to PIC Microcontrollers for Aerospace Systems\n       \n    \n    \n      AERO\n      5254\n      Algorithmic Motion Planning\n       \n    \n    \n      AERO\n      5519\n      Special Topics: Small UAS Dynamics and Control\n       \n    \n    \n      AERO\n      5728\n      Small UAS GNC\n       \n    \n    \n      AERO\n      6044\n      Advanced State Estimation\n      Prereq AERO 5044\n    \n    \n      AERO\n      6216\n      Human Operation of Aerospace Vehicles\n       \n    \n    \n      AERO\n      6519\n      Special Topics: System ID for Control\n       \n    \n    \n      AERO\n      6519\n      Special Topics: Verification and Control of Stochastic Systems\n       \n    \n    \n      AERO\n      6519\n      Special Topics: Safe Autonomy Amid Uncertainty\n       \n    \n    \n      CSCI\n      5302\n      Advanced Robotics\n       \n    \n    \n      CSCI\n      5314\n      Dynamic Models in Biology\n       \n    \n    \n      CSCI\n      5423\n      Biologically-inspired Multi-Agent Systems\n       \n    \n    \n      CSCI\n      5446\n      Chaotic Dynamics\n       \n    \n    \n      CSCI\n      7000\n      Robotic Manipulation\n       \n    \n    \n      CSCI\n      7000\n      Physical HRI and Control\n       \n    \n    \n      ECEN\n      5008\n      Special Topics: Constrained Control\n       \n    \n    \n      ECEN\n      5138\n      Control Systems Analysis\n      Same as MCEN 5228-011 (cross-listed)\n    \n    \n      ECEN\n      5448\n      Advanced Linear Systems\n       \n    \n    \n      ECEN\n      5458\n      Sampled Data and Digital Control Systems\n       \n    \n    \n      ECEN\n      5488\n      Special Topics: Geometric Control Theory\n       \n    \n    \n      ECEN\n      5612\n      Random Processes for Engineers\n       \n    \n    \n      ECEN\n      5678\n      Coordinated Control of Multi-agent Systems\n       \n    \n    \n      ECEN\n      5738\n      Theory of Nonlinear Systems\n       \n    \n    \n      MCEN\n      5115\n      Mechatronics and Robotics I\n       \n    \n    \n      MCEN\n      5228-011\n      Special Topics in Mechanical Engineering: Feedback Control\n      Same as ECEN 5138 (cross-listed)\n    \n    \n      MCEN\n      5228-023\n      Special Topics in Mechanical Engineering: Linear Systems\n       \n    \n    \n      MCEN\n      5228\n      Special Topics: Advanced Dynamics\n       \n    \n    \n      MCEN\n      5228\n      Special Topics: Biohybrid Robotics\n       \n    \n    \n      ROBO\n      5000\n      Introduction to Robotics\n       \n    \n    \n      ROBO\n      5009\n      Advanced Robotics\n      Prereq ROBO 5000 or courses in controls/estimation\n    \n  \n\n",
    "url": "/posts/classes/"
  },
  
  {
    "title": "Lab Computing",
    "excerpt": "\n",
    "content": "Lab Computing Resources\nAccess\nIn order to access ADCL servers, follow the steps below.\n\n\n  Download and set up CU’s VPN if you haven’t already.\n  Connect to CU’s network via the VPN.\n  ssh into the server, generally with the following syntax: &lt;username&gt;@&lt;server_name&gt;.colorado.edu where &lt;username&gt; and &lt;server_name&gt; are replaced with your username and the name of the server being accessed.\n  Enter your password and use the commandline interface as you would locally.\n\n\nTo setup remote connections in VS Code, complete the above steps if you haven’t already set up remote access and install the Remote-SSH extension and follow the steps here. Note the VPN must be connected to use remote access in VSCode.\n\nSession Management\nUse users to see a list of currrently logged-in users.\n\nUse htop to keep track of which system resources are being used and by whom. This is especially important to avoid running multiple resource intesive processes at once. If there are current procecsses using many resources, check with the owner before starting additional processes to avoid interfering with each other’s processes.\n\nLikewise, before generating or downloading large files, check that there is sufficient space on the machine and move (your) old files to the storage drive as needed to free up space. ncdu is a useful utility for seeing file/folder sizes of the current directory. Run this from /home or the root directory (/) to see additional disk usage information. df (specifically df -h) is useful for viewing disk partition use.\n\nPersisent Sessions\nTmux is useful for keeping remote processes running without requiring an active connection to the server. See this tutorial for more information. Julia in VS Code has documentation on enabling persistent sessions within VS Code with Tmux. Please update the Tmux Session Name field to include your username to avoid naming conflicts. See this forum post for details on how to end a Tmux Julia session in VSCode, as this is different from the usual terminal behavior.\n\nUsing Julia\nJuliaup is a Julia version manager. Installing this in your user folder ensures control over your Julia versions and packages. You may need to edit ~/.profile to ensure the Juliaup path is at the top of the list if you have issues launching Julia with Juliaup.\n\nGithub\nFollow instructions for SSH setup to enable cloning of private repositories.\n\nStorage\nPlease limit main drive usage to 65 GB per user. The system has a 1.8 TB storage drive which is located at /media/storage. The system does not mount the drive automatically, so mounting the drive on restart of the machine may be necessary. It may be mounted using the command sudo mount /dev/sdb1 /media/storage. Note that you may need to change directories after mounting the drive for its contents to show.\n\nFiles may be moved with the mv command. Note you may need to use sudo mv.\n\nSystem Administration\nSee ADCL System Administration\n\nCU Computing Resources\nAlpine\nAlpine is a well documented CU-wide SLURM cluster. To get started with Julia on Alpine:\n\n\n  Make an account\n  You may need to wait for some time between making an account and logging in for the first time, the documentation says ~15 minutes\n  Go to the OnDemand page\n    \n      If you get an error, try either clearing your cache/cookies or starting an incognito window\n    \n  \n  Request an interactive session\n    \n      Interactive Apps (top of screen) -&gt; Jupyter Session\n      Check “Use JupyterLab”\n      Preset configuration “4 cores, 4 hours”\n      Launch\n      This should take less than a minute to queue and initialize\n    \n  \n  Install and configure Julia\n    \n      Click the host link i.e. &gt;_c3cpu-c15-u34-3.rc.int.colorado.edu\n      Either save and execute the following bash script, or run line by line. This may take several minutes to run\n        #!/bin/bash\necho export JULIA_DEPOT_PATH=/projects/$USER/.julia &gt;&gt; ~/.bashrc\necho export JULIAUP_DEPOT_PATH=/projects/$USER/.julia &gt;&gt; ~/.bashrc \nsource ~/.bashrc\ncurl -fsSL https://install.julialang.org | sh\nsource ~/.bashrc\njulia --threads auto -e 'import Pkg; Pkg.add(\"IJulia\"); using IJulia; IJulia.installkernel(\"Julia\", \"--threads=auto\"; env=Dict(\"JULIA_DEPOT_PATH\"=&gt;\"/projects/\" * ENV[\"USER\"] * \"/.julia\"))'\n        \n      \n    \n  \n  Log out from your shell and close the tab\n  Back on the OnDemand page “Connect to Jupyter”\n  In JupyterLab\n    \n      File -&gt; Open from Path… -&gt; /home/{Username} i.e. /home/jawa5671\n      Create a Julia notebook\n      println(“Hello world!”)\n    \n  \n\n\nNote: You may need to re-run the IJulia.installkernel function when you install a new version of Julia\n",
    "url": "/posts/compute/"
  },
  
  {
    "title": "List of Conferences",
    "excerpt": "\n",
    "content": "To add more information, edit this file: https://github.com/CU-ADCL/CU-ADCL.github.io/blob/main/posts/conferences.md\n\n\n  \n    \n      Title\n      Usual Month\n      Usual Deadline\n    \n  \n  \n    \n      ICRA\n      May\n      Sep\n    \n    \n      ACC\n      June/July\n      Sep\n    \n    \n      AAAI\n      February\n      Aug\n    \n    \n      ICAPS\n      June\n      Nov\n    \n    \n      AAMAS\n      May\n      Nov\n    \n    \n      IJCAI\n      July\n      Jan\n    \n    \n      RSS\n      July\n      Feb\n    \n    \n      UAI\n      Aug\n      Feb\n    \n    \n      IROS\n      Oct\n      Mar\n    \n    \n      CDC\n      Dec\n      Mar\n    \n    \n      SciTech\n      Jan\n      Jun (abstract)\n    \n    \n      ITSC\n      Sep\n      Mar\n    \n    \n      CoRL\n      Nov\n      Jul\n    \n    \n      NeurIPS\n      Dec\n      May\n    \n    \n      IEEE Aerospace\n      March\n      Oct\n    \n    \n      IEEE Intelligent Vehicles Symposium\n      July\n      Feb\n    \n    \n      WAFR (Workshop on the Algorithmic Foundations of Robotics)\n      June/July\n      Feb\n    \n    \n      DASC\n      Sept\n      March (abstract)\n    \n    \n      Safety, Security, and Rescue Robotics (SSRR)\n      October\n      June\n    \n    \n      ICUAS\n      June\n      Feb\n    \n    \n      L4DC\n      June\n      Dec\n    \n    \n      CHI\n      Apr/May\n      Sept\n    \n    \n      ICML\n      Jul\n      Jan\n    \n    \n      RLC\n      Aug\n      Feb\n    \n    \n      AISTATS\n      May\n      Oct\n    \n  \n\n",
    "url": "/posts/conferences/"
  },
  
  {
    "title": "Diffusion Models: A New Playground for Reinforcement Learning",
    "excerpt": "\n",
    "content": "Reinforcement learning (RL) has achieved remarkable progress in solving complex decision-making problems, from playing Go and mastering Atari games (Schrittwieser et al., 2019) to robotic control. Yet, one major limitation remains — sample inefficiency. RL agents often require millions of interactions with their environments to learn effective policies. In the real world, such exhaustive trial-and-error is often impractical or prohibitively time expensive.\n\nTo address this, researchers have turned to world models — generative models that learn to simulate environments. Instead of interacting with the real world at every step, an agent can learn by imagining outcomes within its learned world model. This idea, popularized by works like World Models (Ha &amp; Jü, 2018), has inspired a line of research into using generative modeling as a core component to train RL agents.\n\n\n  \n  \n  Figure 1: A World Model. Credit to (Scott McCloud, 1993) .\n\n\nOver time, a variety of generative modelling approaches have been explored to serve as effective world models, including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Flow-based models.\n\n\n  VAEs are stable and easy to train but rely on a surrogate loss, which can lead to blurry reconstructions (Kingma &amp; Welling, 2013). This can be limiting when high-fidelity state predictions are required for planning in RL.\n  GANs can produce highly realistic samples, but their adversarial training often leads to instability and mode collapse, resulting in limited diversity — a critical drawback when trying to capture all possible environment states (Goodfellow et al., 2014) (Martí, 2017) .\n  Flow-based models learn bijective mappings between data and latent spaces, allowing for exact likelihood estimation and stable training (Rezende &amp; Mohamed, 2015). However, they require carefully designed architectures to maintain reversibility, which can limit flexibility and scalability in modeling complex, high-dimensional environments.\n\n\nWhile VAEs, GANs, and flow models have all been explored as world models, each involves trade-offs that can affect their use in RL. This has led researchers to investigate diffusion models, a newer class of generative models with a different approach to modeling data distributions.\n\nThe Basics of Diffusion Models\n\nDiffusion models are inspired by non-equilibrium thermodynamics, where systems naturally evolve from low-entropy (ordered) states to high-entropy (disordered) states. They define a Markov chain of diffusion steps that gradually add random noise to data, mimicking the physical process of diffusion. The model then learns to reverse this process — transforming noise back into structured data — effectively moving from a high-entropy to a low-entropy state.\n\n\n  \n  \n  \n  Figure 2: Example of a trained diffusion model for modeling a 2D swiss roll data. Credit to (Sohl-Dickstein et al., 2015)\n  .\n\n\nThrough this learned denoising dynamics, diffusion models can generate realistic data samples by starting from random noise and iteratively reconstructing the underlying structure of the data distribution. Conceptually, this can be seen as an encoding–decoding process, like VAEs or flow-based models, where the forward diffusion “encodes” data into a noisy latent and the reverse process “decodes” it back into structured samples. However, unlike VAEs or flows, the forward procedure is fixed (not learned), and the latent variables have the same dimensionality as the original data rather than being compressed into a smaller latent space.\n\nThe forward and backward processes in diffusion models can be formulated in both discrete and continuous forms. We will briefly mention the discrete form as it is the original historical formulation, but our focus will be on the continuous form, which is more generic and better captures complex, high-dimensional dynamics.\n\nIn the discrete-time formulation, the forward process gradually adds noise to a data sample $x^0$ over $T$ timesteps, producing a sequence ${x^1, x^2, \\dots, x^T}$.\nAt each step $\\tau$, noise is added according to a predefined schedule $\\beta_\\tau$:\n\n[x^\\tau = \\sqrt{1 - \\beta_\\tau} \\, x^{\\tau-1} + \\sqrt{\\beta_\\tau} \\, \\epsilon_\\tau, \\quad \\epsilon_\\tau \\sim \\mathcal{N}(0, I)]\n\nThe backward process learns to reverse this noising process by estimating the conditional distribution $p_\\theta(x^{\\tau-1} \\mid x^\\tau)$, typically parameterized by a neural network.\n\nIn the continuous-time formulation, the forward process is modeled as a stochastic differential equation (SDE), which smoothly diffuses the data into noise:\n\n[dx = f(x, \\tau) \\, d\\tau + g(\\tau) \\, dW_\\tau]\n\nwhere $f(x, \\tau)$ represents a predefined deterministic drift term, $g(\\tau)$ a predefined diffusion coefficient, and $dW_\\tau$ denotes a Wiener process (Brownian motion).\n\nThe backward process then corresponds to a reverse-time SDE, which removes noise by following the time-reversed dynamics:\n\n[dx = [f(x, \\tau) - g(\\tau)^2 \\nabla_x \\log p_\\tau(x)] \\, d\\tau + g(\\tau) \\, d\\bar{W}_\\tau]\n\nwhere $d\\bar{W}_{\\tau}$ represents the reverse-time Wiener process and $\\nabla_x \\log p_\\tau(x)$ the score function of the noisy distribution at time $\\tau$.\n\nThe score function $\\nabla_x \\log p_\\tau(x)$ represents the gradient of the log-probability of the data at time $\\tau$, pointing toward regions of higher likelihood. It adjusts the drift $f(x, \\tau)$ in the reverse-time SDE, guiding $x$ to produce realistic samples.\n\nSince the distribution $p_\\tau(x)$ is generally unknown, the score function $\\nabla_x \\log p_\\tau(x)$ is also unknown and must be estimated from data. Diffusion models achieve this by leveraging a combination of sampling and learning. A typical approach assumes that the deterministic shift function $f(x, \\tau)$ is affine, which simplifies the mathematics and allows the forward diffusion process to reach any intermediate time $\\tau$ analytically using a Gaussian perturbation kernel $p^{0}_ {\\tau}(x^\\tau \\mid x^0)$. The procedure then proceeds as follows:\n\n\n  Sample clean data $x^0$ from the target distribution.\n  Corrupt the samples by simulating the forward diffusion to obtain $x^\\tau$ at the desired time $\\tau$. If $f$ is affine, this can be done in a single step using the known Gaussian kernel.\n  \n    Train a score model $s_\\theta(x, \\tau)$ to estimate the score function by minimizing a denoising objective such as (Karras et al., 2022):\n\n\\[\\mathbb{E}_{x^0, x^\\tau \\sim p^{0}_{\\tau}} \\left[ \\left\\| s_\\theta(x^\\tau, \\tau) - \\nabla_{x^\\tau} \\log p^{0} _{\\tau}(x^\\tau \\mid x^0) \\right\\|^2 \\right]\\]\n  \n\n\nOnce trained, this score model provides an approximation of $\\nabla_x \\log p_\\tau(x)$ at any time $\\tau$, allowing the backward SDE to iteratively denoise samples and generate realistic data from pure noise.\n\nDiffusion Models for World Modelling\n\nIn the context of world modeling for sequential decision-making, the goal is to learn a generative model of the environment dynamics — that is, to predict the next observation of the system given past observations and actions. Formally, we aim to model a conditional generative distribution:\n\\(p_\\theta(x_{t+1} \\mid x_{\\le t}, a_{\\le t}),\\)\n\nwhere $x_{t+1}$ is the next (unknown) observation, $x_{\\le t} = {x_0, \\dots, x_t}$ are past observations, and $a_{\\le t} = {a_0, \\dots, a_t}$ are past actions. Unlike unconditional diffusion models, here the diffusion model must condition on the history of observations and actions to accurately predict the future.\n\nTo train a diffusion model for world modeling, we first need to collect trajectories $(x_0, a_0, x_1, a_1, \\dots, x_t, a_t, x_{t+1})$ from the real environment to obtain clean next observations $x_{t+1}$ as targets. Once we have these trajectories, we treat $x_{t+1}$ as the “clean” data and corrupt it with a Gaussian perturbation kernel to produce a noisy sample $x_{t+1}^\\tau$. The model then learns a conditional score function $s_\\theta(x_{t+1}^\\tau, \\tau \\mid x_{\\le t}, a_{\\le t})$ that estimates the gradient of the log-probability of the noisy next observation. The training objective is the following loss:\n\n[\\mathcal{L}(\\theta) = \\mathbb{E}{x{t+1}, \\, x_{t+1}^\\tau \\sim p^0\\tau} \\Big[ | s\\theta(x_{t+1}^\\tau, \\tau \\mid x_{\\le t}, a_{\\le t}) - \\nabla_{x_{t+1}^\\tau} \\log p^0\\tau(x{t+1}^\\tau \\mid x_{t+1}) |^2 \\Big],]\n\nwhich encourages the model to reverse the corruption process and recover $x_{t+1}$ conditioned on past observations and actions.\n\nOnce the diffusion model is trained, it can be used as a generative model for decision-making. In this setting, the next observation $x_{t+1}$ is typically unknown and is initially represented either as Gaussian noise or a prior estimate. The model then iteratively denoises this sample, step by step, using the reverse-time SDE, conditioned on past observations and actions. This procedure generates a realistic prediction of the next state, which can be used to train a RL agent.\n\n\n  \n  \n  Figure 3: The diffusion model takes into account the agent's action and the previous frames to simulate the environment response. Credit to (Alonso et al., 2024).\n\n\nIterative Training of the Reinforcement Learning Agent and the Diffusion Model\n\nIn general, diffusion models and reinforcement learning (RL) agents are not trained fully separately but rather in an iterative loop, where both models are improved in alternating phases. The typical procedure involves three main steps:\n\n\n  \n    Collect real data: The RL agent interacts with the real environment collecting trajectories of observations, actions,rewards, and termination signals in a replay buffer.\n  \n  \n    Train the world model: A diffusion-based world model is trained using all data from the replay buffer. In addition to modeling the next observation, auxiliary components such as a reward model and a termination model are included to fully capture the environment dynamics.\n  \n  \n    Train the RL agent in imagination: Once the world model is trained, it replaces the real environment during policy optimization. The agent is trained by imagining trajectories—simulating rollouts within the learned model—using the predicted next observations, rewards, and terminations.\n  \n\n\nThese three steps are repeated in a loop, allowing the world model to continuously refine its understanding of the environment while the RL agent progressively improves its policy through imagined experiences.\n\nA Promising Example\n\nA recent paper, Diffusion for World Modeling: Visual Details Matter in Atari (Alonso et al., 2024), presents DIAMOND, a RL-agent trained on a diffusion-based world model that achieves impressive performance on the Atari 100k benchmark. This benchmark evaluates agents across 26 Atari games, where each agent is allowed only 100k real environment interactions—roughly equivalent to two hours of human gameplay—to train its world model. For context, traditional RL agents without world models are typically trained in the environment for up to 50 million steps, meaning DIAMOND needs 500× less interactions with the environment.\n\nDIAMOND is compared against several state-of-the-art world model agents, including STORM (Zhang et al., 2023), DreamerV3 (Hafner et al., 2023), IRIS (Micheli et al., 2023), TWM (Robine et al., 2023), and SimPle (Kaiser et al., 2019). In aggregate performance, DIAMOND achieves a superhuman mean human-normalized score (HNS) of 1.46, outperforming all previous world model agents, and exceeding human-level performance on 11 out of 26 games. Notably, its interquartile mean (IQM) performance matches that of STORM while surpassing all other baselines, demonstrating consistent performance across games. DIAMOND particularly excels in visually detailed environments such as Asterix, Breakout, and Road Runner, where precise visual modeling directly influences decision-making.\n\n\n  \n  \n  \n  Figure 4: Mean and interquartile mean human normalized scores. Credit to (Alonso et al., 2024).\n  \n\n\nA key comparison is with IRIS, a world model based on a Variational Autoencoder (VAE) architecture. While IRIS generates plausible trajectories, they often suffer from visual inconsistencies between consecutive frames—for example, enemies being rendered as rewards or vice versa. Although these discrepancies may only affect a few pixels, they can drastically alter the agent’s learning process, as reward-related information is critical for policy optimization. In contrast, DIAMOND’s diffusion-based approach produces visually consistent trajectories, more accurately reflecting the true environment. These improvements in visual fidelity directly translate to stronger agent performance across several games.\n\n\n  \n  \n  \n  Figure 5: Consecutive frames imagined with IRIS (left) and DIAMOND (right). The white square highlights the visual inconsistencies.  Credit to (Alonso et al., 2024).\n  \n\n\nOverall, DIAMOND provides compelling evidence that diffusion models can significantly advance world modeling for reinforcement learning, enabling more accurate, visually coherent, and data-efficient policy learning.\n\nLimitations\n\nWhile diffusion models offer promising capabilities for generative world modeling, several limitations remain when applying them within reinforcement learning (RL) settings:\n\n\n  \n    Discrete vs. Continuous Control:\nCurrent implementations are primarily evaluated in discrete action spaces. It remains uncertain how well these methods would generalize to continuous control environments, such as those involving robotic joint torques or accelerations.\n  \n  \n    Limited Temporal Context:\n For computational efficiency, frame stacking is usually necessary. Instead of providing all past observations, the model only receives the last 𝐻 frames, introducing a form of short-term memory. However, this context is limited—information beyond 𝐻 steps is discarded. In scenarios with repetitive frames (e.g., static visual inputs like walls), the model may lose context and generate unrealistic or inconsistent dynamics.\n  \n  \n    Reward and Termination Extraction:\nWhen integrating diffusion models into RL pipelines, it is not straightforward how to derive reward signals or termination conditions directly from the generative representation. For example, in the DIAMOND framework, a separate reward and termination model is used alongside the diffusion model to handle these aspects effectively.\n  \n\n\nReferences\n\n\nAlonso, E., Jelley, A., Micheli, V., Kanervisto, A., Storkey, A., Pearce, T., et al. (2024). Diffusion for World Modeling: Visual Details Matter in Atari. Thirty-eighth Conference on Neural Information Processing Systems. https://arxiv.org/abs/2405.12399\n\n\nGoodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., et al. (2014). Generative Adversarial Nets. Neural Information Processing Systems. https://api.semanticscholar.org/CorpusID:261560300\n\n\nHa, D. R., &amp; Jü (2018). World Models. ArXiv, abs/1803.10122. https://api.semanticscholar.org/CorpusID:4807711\n\n\nHafner, D., Pasukonis, J., Ba, J., Lillicrap, T., &amp; Lillicrap, T. (2023). Mastering Diverse Domains through World Models. ArXiv, abs/2301.04104. https://arxiv.org/abs/2301.04104\n\n\nKaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R. H., Czechowski, K., et al. (2019). Model-Based Reinforcement Learning for Atari. International Conference on Learning Representations. https://arxiv.org/abs/1903.00374\n\n\nKarras, T., Aittala, M., Aila, T., Laine, S., &amp; Laine, S. (2022). Elucidating the Design Space of Diffusion-Based Generative Models. ArXiv, abs/2206.00364. https://api.semanticscholar.org/CorpusID:249240415\n\n\nKingma, D. P., &amp; Welling, M. (2013). Auto-Encoding Variational Bayes. CoRR, abs/1312.6114. https://api.semanticscholar.org/CorpusID:216078090\n\n\nMartí (2017). Towards Principled Methods for Training Generative Adversarial Networks. ArXiv, abs/1701.04862. https://api.semanticscholar.org/CorpusID:18828233\n\n\nMicheli, V., Alonso, E., &amp; Fleuret, F. (2023). Transformers are Sample-Efficient World Models. International Conference on Learning Representations. https://arxiv.org/abs/2209.00588\n\n\nRezende, D. J., &amp; Mohamed, S. (2015). Variational Inference with Normalizing Flows. ArXiv, abs/1505.05770. https://api.semanticscholar.org/CorpusID:12554042\n\n\nRobine, J., Höftmann, M., Uelwer, T., Harmeling, S., &amp; Harmeling, S. (2023). Transformer-based World Models Are Happy With 100k Interactions. ArXiv, abs/2303.07109. https://arxiv.org/abs/2303.07109\n\n\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., et al. (2019). Mastering Atari, Go, chess and shogi by planning with a learned model. Nature, 588, 604 - 609. https://api.semanticscholar.org/CorpusID:208158225\n\n\nSohl-Dickstein, J. N., Weiss, E. A., Maheswaranathan, N., Ganguli, S., &amp; Ganguli, S. (2015). Deep Unsupervised Learning using Nonequilibrium Thermodynamics. ArXiv, abs/1503.03585. https://api.semanticscholar.org/CorpusID:14888175\n\n\nZhang, W., Wang, G., Sun, J., Yuan, Y., Huang, G., &amp; Huang, G. (2023). STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning. Advances in Neural Information Processing Systems, 36. https://arxiv.org/abs/2310.09615\n\n\nMcCloud, S. (1993). Understanding Comics: The Invisible Art. Tundra Publishing. https://en.wikipedia.org/wiki/Understanding_Comics\n",
    "url": "/posts/dmupp-2025/diffusion-world-models/"
  },
  
  {
    "title": "Generative Models hate this one trick to mitigate Observation Complexity",
    "excerpt": "\n",
    "content": "Abstract\n\nThe “Generative Model” is often defined as a Neural Network (NN) that allows for the querying of some input and receiving some output. In the case of a Generative Observation Model, it defines $p\\left(o \\, \\mid \\, h_t\\right)$. To those with a keen eye, you might realize that this begets modeling every pixel, shadow, and irrelevant texture in the case of a visual observation. Discriminative Particle Filter Reinforcement Learning (DPFRL) for Complex Partial Observations says “why bother?” and instead proposes the idea of using a set of learned latent states and weights to approximate the belief. By matching particles to observations and scoring them, DPFRL instead skips the need for messy reconstructions. Add on these magical “Moment Generating Functions” (MGFs) and we have a system better equipped to handle visually complex environments.\n\nGenerative Models Just Can’t Cut It\n\nA Generative Model (in the case of Reinforcement Learning) is typically a Neural Net (NN) that defines the probability of seeing a specific observation given some latent state: $p\\left(o \\, \\mid \\, h_t\\right)$. These generative models must essentially model every variation of every feature for every environment. In the case of an image, this means considering every pixel, even if said pixel contains irrelevant data. This yields what is called a “Complex Observation Environment”.\n\nGenerative Models have difficulty with this as they inherently consider these irrelevant visual complexities. They’re like me in the Lego store, unable to resist purchasing something. While a generative model may do fine in an environment similar to the one that it was trained in; turn the lights off and you might suddenly find your algorithm struggling to perform just as well. This is because the agent receives an observation (while possibly of an environment that it has seen before) that it perceives as different due to the different pixel values or latent state reconstruction. Imagine if, upon turning the lights off in your house, you suddenly believed you were in a completely different country. That would probably make you act differently than you would if you realized you were still in your house.\n\nThis leads to a desire for some Observation Model that is more capable of ignoring these irrelevant details. What would this observation model look like, however? If we consider ourselves as a little soldier that needs to localize itself; using a generative model is like knowing (or having to know) every possible environment under every possible circumstance and then get a probability distribution of where it is with such knowledge. This is understandably unreasonable, so what if we instead just looked at features of the environment we are in and made a guess from there.\n\nToday’s Hero: The Discriminative Particle Filter\n\n\n\n\nThe authors of Discriminative Particle Filter Reinforcement Learning (DPFRL) propose the discriminative belief update. A discriminative update “discriminates” by which particle better fits the current observation. This is done by the use of two learned NNs trained end-to-end.\n\nThe first network is an Observation-Conditioned Transition Function $h_t^i \\sim f_{trans}\\left(h_{t - 1}^i, a_t, o_t\\right)$. This network outputs / proposes a new latent state that it believes is most likely given the previous latent state, current action taken and observation currently seen. Back to our little soldier analogy; this is like the soldier considering what it sees and guessing where it could be. Is it a really green or lush environment? Perhaps it’s in a soil-rich ecosystem or near water? Is it really rocky and dry? Perhaps it’s at altitude or in a hot environment? This is what the observation conditioned transition function does. It considers its environment (and its previously guessed location, as well as what action it just took) and guesses where it is.\n\nThe second network is a Compatibility Function $f_{obs}\\left(h_t^i, o_t\\right)$. The previous “proposed new latent state” is then assigned a weight by $f_{obs}$ given said latent state and current observation (multiplied by the previous weight and scaling factor $\\eta$). Again, back to the little soldier analogy; this is like saying “Ok, I think I’m here now… does that actually align with what I’m seeing?”\n\nConsider the following example: Our little soldier is in an icy, cold environment with penguins everywhere. The soldier sails North and reaches a semi-lush / arid environment with kangaroos. The transition function $f_{trans}\\left(h_{t - 1}^i, a_t, o_t\\right)$ takes into account that the soldier moved North, was previously somewhere cold and icy and is now seeing someplace arid. It then proposes a set of new locations (latent state particles), let’s say: [Southern Africa, Southern South America, Southern Australia]. The compatibility function $f_{obs}\\left(h_t^i, o_t\\right)$ then weights each of these proposed latent state particles according to what it is seeing. It sees a semi-lush / arid environment so perhaps it could be any of the three, but the kangaroos align with a high weight for Southern Australia. Thus, each latent state has weights perhaps as such: {{Southern Africa, 0.2}, {Southern South America, 0.1}, {Southern Australia, 0.7}}. This set of latent states and associated weights define our set of particles.\n\nIn the last Discriminative Particle Filter step, our particles are regenerated (as is common in particle filters) to prevent particle degeneration. The unnormalized particle weights found from the compatibility function are normalized, and these particles are passed back into the filter to be used again for the next step. The former two steps are wrapped into a “Soft-Resampling” step as called by the original authors. The final step is to form our particles into a belief used for our policy and value updates.\n\nPolicy &amp; Value Update\n\nThe policy update step is separated into two routines: the belief formulation and policy and value updates. The belief is formulated using the mean latent state $\\left(\\bar{h}_t\\right)$ and Moment Generating Function (MGF) features. These MGF features summarize the belief by evaluating the weighted latent states at learned locations, capturing higher-order information. This higher-order information is more descriptive than just the mean, which is useful in complex environments.\n\nThis is like our little soldier considering all particles (proposed locations), picking a final value (mean) and extracting high-order features from the set of all of them (such as flora and / or fauna in each proposed location) to make a final decision about where it is (its belief).\n\nThese features combine to form a permutation-invariant belief vector $\\left[\\bar{h}_t, M_t^{1:m}\\right]$. This belief vector is then used to determine our policy $\\pi\\left(a_t \\, \\mid \\, b_t\\right)$ and value $V\\left(b_t\\right)$.\n\nSo… Why Does This Work?\n\n\n\nThe short version is that DPFRL optimizes for complex environments, not reconstruction; it only learns to recognize observation features that help pick good actions. The Observation-Conditioned Transition Function $f_{trans}\\left(h_{t - 1}^i, a_t, o_t\\right)$ steers particles toward latent states that can actually explain what was just seen considering the agent dynamics. The Compatibility Function $f_{obs}\\left(h_t^i, o_t\\right)$ assigns unnormalized scores to the latent state particles relative to its learned ranking. These scores are normalized along with particle regeneration during a Soft-Resampling step.\n\nDPFRL cleverly turns the problem of modeling the full image distribution $p\\left(o \\, \\mid \\, h_t\\right)$ into an easier one: learn to propose plausible hypotheses from what you just saw and score them for decision relevance. On benchmarks where observations are visually complex, DPFRL maintains “state-of-the-art” scores while baselines using a generative model struggle as they see observations of the same environment under different conditions as completely different environments.\n\nDPFRL only needs to learn “does this particle match what I’m seeing for decision purposes?” instead of “can I reconstruct every pixel, including that coffee stain?”; a way easier problem to solve. On benchmarks like Flickering Atari Games (where frames randomly go blank) and Natural Flickering Atari Games (the same as the prior except with random YouTube videos as backgrounds), DPFRL substantially outperforms methods using generative models. TL;DR: Instead of painting a masterpiece to prove you’re in a museum, just check if the important features of what you see match “you’re in a museum”.\n\nFAQ\n\nI don’t like tuning hyperparameters… How many knobs does DPFRL have to tune?\n\n\n  Not many actually!\n\n  \n    Particle count $K$: more increases compute and memory usage, but better handles multi-modality and approximates of the posterior distribution.\n    Soft‑Resampling $\\alpha$: higher leads to importance sampling; lower leads to more exploration to avoid particle collapse.\n    MGF feature count $m$: captures more higher-order information from the particle moments.\n  \n\n\nWhat are the trade‑offs?\n\n\n  You don’t get the additional reconstruction signal that some generative models can use, which sometimes helps supplement learning in simpler settings. Similarly, with too few particles or extremely complex visuals, the model can still struggle to maintain a sharp belief.\n\n\nWhat are MGF features in plain English?\n\n\n  Think of them as quick “probes” of the belief that capture not just the average guess, but also how spread out or lopsided the guesses are.\n\n\nWhat if my observations are simple?\n\n\n  The performance gap narrows; generative models, mean‑only belief summaries, only 1 particle and more may perform similarly, but DPFRL remains a solid, structured baseline comparable to generative models nonetheless.\n\n\nResources\n\nConsidering looking into DPFRL for your use case? Perhaps you want to learn about specific algorithmic implementation details to further your understanding of the high-level topics. Here’s a list of nice resources for you that I found useful:\n\n\n  Original paper: Discriminative Particle Filter Reinforcement Learning (DPFRL) for Complex Partial Observations\nAuthor GitHub implementation: DPFRL\n\n",
    "url": "/posts/dmupp-2025/dpfrl-generative-models/"
  },
  
  {
    "title": "Communicating near the edge of your knowledge",
    "excerpt": "\n",
    "content": "\n\nAs researchers, it is our job to be operating on the edge between what we know and what we do not yet know. Moreover, to be effective researchers, we must do this in partnership with other people, and that means we need to communicate effectively in this realm.\n\nFor Coloradans, perhaps a good metaphor is walking near a steep slope in the fog without knowing which rocks are stable. There are some areas where we have very solid knowledge that we can put weight on with confidence. Sometimes we need to test the rocks and it is unwise to put weight on rocks that we are not sure about, but making forward progress is an important goal.\n\nThe following are some tips for communicating when you are near the edge of your knowledge:\n\nThings to do:\n\n\n  Prepare for conversations. Think hard about what you should know before a conversation and investigate things you are unsure of.\n  Talk about the edge. Clearly state what you do and don’t know, for example “I know that value iteration converges in finite discrete Markov decision processes, but I don’t know what will happen in continuous spaces.”\n  Hedge precisely. If you are not sure of something or only have an approximate idea, qualify it with hedges like “almost” or “something like” to communicate the lack of precision in your knowledge. I like to even use probabilities, e.g. “I am about 80% sure that SARSOP applied to an MDP will be less efficient than value iteration.”.\n  Clarify definitions. A great way to build understanding is to clarify definitions of words and phrases that you and your partner are using but perhaps in different ways, e.g. “When you call something a POMDP, is the reward a function of the state and action, or can it be any function of the belief, like entropy?”.\n  Repeat to test understanding. If your partner says something and you are not sure exactly what it means, you can try to repeat it back to them in your own words. This will almost always resolve or at least elucidate misunderstandings.\n  Try to make headway by reasoning out loud. If you are unsure of something in a conversation, try to see if you can reach a conclusion. One good way to do this is reason out loud. This informs your partner that you are working through something, gives them the opportunity to help, and helps outline the edges of your knowledge as you go.\n\n\nThings to avoid:\n\n\n  Pretending that you know something that you do not. This seems obvious, but it is extremely tempting when someone expects you to know about it. It is much better to stop and ask them to explain. You can even say something like, “I have heard of that before, but don’t know the details”. Almost all of the time, people will be extremely happy to explain it to you, and your mind is primed for learning at this moment.\n  Excessive hedging. When you are nervous, it can be tempting to use hedges on things that you are sure of, e.g. “A POMDP where the observation is the state is sort of an MDP.”, or use hedges to just fill space in all of your sentences. This makes communication more difficult to understand because your partner cannot distinguish between what you are certain about and what you are actually unsure of. If you find yourself hedging when it should not be necessary, that can guide your later research; minimizing hedging may be a good objective function for self-teaching.\n\n",
    "url": "/posts/edge-of-knowledge/"
  },
  
  {
    "title": "Autonomous Decision and Control Lab",
    "excerpt": "We study and develop autonomy with an uncertainty-first approach.\n",
    "content": "You can find out more about the ADCL using the Research, Publications, and People links above. Our group meetings are open to the public.\n\nNews\n\n    \n      \n      \n      \n      \n        30 Jun 2024\n        Our paper \"Feasibility-Guided Safety-Aware Model Predictive Control for Jump Markov Linear Systems\n\" has been accepted for presentation at the International Conference on Intelligent Robots and Systems (IROS) 2024.\n\n      \n    \n      \n      \n        26 Apr 2024\n        Our paper \"Sound Heuristic Search Value Iteration for Undiscounted POMDPs with Reachability Objectives\" has been accepted for presentation at the Conference on Uncertainty in Artificial Intelligence (UAI) 2024.\n\n      \n    \n      \n      \n        26 Apr 2024\n        Our paper \"Recursively-Constrained Partially Observable Markov Decision Processes\" has been accepted for presentation at the Conference on Uncertainty in Artificial Intelligence (UAI) 2024.\n\n      \n    \n      \n      \n        28 Feb 2024\n        Our workshop paper on \"Leveraging Counterfactual Paths for Contrastive Explanations of POMDP Policies\" has been accepted for presentation at the Explainability for Human-Robot Collaboration (X-HRI) Workshop at HRI 2024.\n\n      \n    \n      \n      \n        14 Feb 2024\n        Himanshu’s submission to the Doctoral Consortium at AAMAS-2024, entitled ‘Efficient Continuous Space BeliefMDP Solutions for Navigation and Active Sensing’ has been accepted.\n\n      \n    \n      \n      \n        12 Feb 2024\n        Our paper \"Human-Centered Autonomy for Autonomous sUAS Target Search\" has been accepted for presentation at the IEEE International Conference on Robotics and Automation (ICRA) 2024.\n\n      \n    \n      \n      \n        19 Jan 2024\n        Our paper \"Cieran: Designing Sequential Colormaps with a Teachable Robot\" has been accepted to the Conference on Human Factors in Computing Systems (CHI) 2024.\n\n      \n    \n      \n      \n        16 Nov 2023\n        Our paper on \"Sampling-based Reactive Synthesis for Nondeterministic Hybrid Systems\" has been accepted for a journal publication in the Robotics and Automation Letters (RA-L)\n\n      \n    \n      \n      \n        30 Oct 2023\n        Our paper on \"Investigation of risk-aware MDP and POMDP contingency management autonomy for UAS\" has been accepted for a journal publication in the Journal of Aerospace Information Systems\n\n      \n    \n      \n      \n        27 Oct 2023\n        We published a new Julia package that can create a probabilistic roadmap (PRM) for any given environment.\n\n      \n    \n      \n      \n        28 Sep 2023\n        We published a new Julia package that implements the Hybrid A* algorithm.\n\n      \n    \n      \n      \n        21 Aug 2023\n        The ADCL will be part of a major new effort to improve space domain awareness in cislunar space, collaborating with researchers at CU, Texas A&amp;M, and Georgia Tech.\n\n      \n    \n      \n      \n        10 May 2023\n        We are organizing a workshop on inference and decision making at RSS 2023 on July 10.\n\n      \n    \n      \n      \n        17 Apr 2023\n        Our paper on \"Optimality Guarantees for Particle Belief Approximation of POMDPs\" has been accepted for a journal publication in the Journal of Artificial Intelligence Research (JAIR).\n\n      \n    \n      \n      \n        12 Apr 2023\n        Our paper on \"Explanation through Reward Model Reconciliation using POMDP Tree Search\" has been accepted for presentation at the IEEE Int’l. Conference on Assured Autonomy (ICAA) 2023\n\n      \n    \n      \n      \n        16 Jan 2023\n        Our paper on \"Planning with SiMBA: Motion Planning under Uncertainty for Temporal Goals using Simplified Belief Guides\" has been accepted for presentation at IEEE Int’l Conf. on Robotics and Automation (ICRA) 2023.\n\n      \n    \n      \n      \n        10 Jan 2023\n        Our paper on \"Navigation between initial and desired community states using shortcuts\" has been accepted for a journal publication in Ecology Letters.\n\n      \n    \n      \n      \n        01 Oct 2022\n        Tyler and Jackson presented their respective research on imperfect information games and adaptive stress testing at the AMOS conference.\n\n      \n    \n      \n      \n        22 Aug 2022\n        We are organizing a workshop on game theory for CoRL 2022 on Dec 15.\n\n      \n    \n      \n      \n        04 Aug 2022\n        We released the POMDPTools package, consolidating important tools for POMDPs.jl\n\n      \n    \n      \n      \n        16 Jul 2022\n        Our paper on \"Automaton-Guided Control Synthesis for Signal Temporal Logic Specifications\" has been accepted for presentation at CDC 2022.\n\n      \n    \n      \n      \n        11 Apr 2022\n        Himanshu successfully defended his Master’s Thesis. Congratulations Himanshu!\n\n      \n    \n      \n      \n        07 Apr 2022\n        Ben Kraske and John Tucker have received the NSF GRFP award. Congratulations Ben and John!\n\n      \n    \n      \n      \n        31 Jan 2022\n        Our paper on \"Gaussian Belief Trees for Chance Constrained Asymptotically Optimal Motion Planning\" has been accepted for presentation at ICRA 2022.\n\n      \n    \n      \n      \n        20 Dec 2021\n        Our paper on \"Intention-Aware Navigation in Crowds with Extended-Space POMDP Planning\" has been accepted for presentation at AAMAS 2022. (Slides)\n\n      \n    \n      \n      \n        01 Dec 2021\n        Our website is finally up!\n\n      \n    \n\n\n",
    "url": "/"
  },
  
  {
    "title": "People",
    "excerpt": "\n",
    "content": "Current Members\n\nFaculty\n\n\n\n\n\n\n\n\n\n\n\nZachary Sunberg\n\n\n    Faculty Member\n    zachary.sunberg@colorado.edu\n\n\n\n    \n    Research Interests: POMDPs, Aerospace autonomy, Stochastic games, Applying autonomous decision making under uncertainty in new contexts\n\n    \n    \n        Detailed Bio\n        \n            I’m an Assistant Professor in the Ann and H.J. Smead Aerospace Engineering Sciences Department. I earned Bachelors and Masters degrees in Aerospace Engineering from Texas A&amp;M University and a PhD in Aeronautics and Astronautics at Stanford University in the Stanford Intelligent Systems Lab. Before joining the University of Colorado faculty, I was a postdoctoral scholar at the University of California, Berkeley in the Hybrid Systems Lab. My research is focused on decision making under uncertainty to enable safe and efficient autonomous vehicle operation. My recent research has focused on developing online algorithms for approximately solving partially observable Markov decision processes (POMDPs) and games with continuous or hybrid state, action, and observation spaces. In my free time, I enjoy skiing, trail running (really more like trail slogging), volunteering at my church, and following long Wikipedia rabbit trails.\n\n        \n    \n\n\n\nPostdocs\n\n\n\n\n\n\n\n\n\n\n\nOfer Dagan\n\n\n    Postdoctoral Associate\n    ofer.dagan@colorado.edu\n\n\n\n    \n    Research Interests: Multi-robot systems, Sensor data fusion, Bayesian inference, Probabilistic graphical models, Decision making under uncertainty\n\n    \n    \n        Detailed Bio\n        \n            I am a new postdoc at ADCL, starting January 2024. I hold a Ph.D. in aerospace engineering sciences from CU Boulder. Prior to coming to CU Boulder, I worked in an R&amp;D aerospace industry for about 10 years as a research engineer and team leader.\nI hold a B.S. degree in aerospace engineering (2010, summa cum laude), and an M.S. degree in mechanical engineering (2015, cum laude), from the Technion – Israel Institute of Technology, Haifa, Israel.\nMy research interests include theory and algorithms for decentralized Bayesian reasoning and decision making under uncertainty in heterogeneous autonomous systems.\n\n        \n    \n\n\n\nPhD Students\n\n\n\n\n\n\n\n\n\n\n\nBen Kraske\n\n\n    PhD Student\n    benjamin.kraske@colorado.edu\n\n\n\n    \n    Research Interests: POMDPs, Explainable Artificial Intelligence (XAI), Aerospace Autonomy for Safety\n\n    \n    \n        Detailed Bio\n        \n            Hi, I’m Ben. I’m currently pursuing my interest in flight as a PhD student in aerospace engineering at the University of Colorado Boulder. I completed my bachelor’s degree in mechanical engineering at George Fox University, outside Portland, Oregon. My current research interests center around explainable decision making under uncertainty and the development of autonomous systems for safety onboard manned and unmanned aircraft. When I’m not studying or working on research, I like to get outside and bike, rock climb, or play some soccer.\n\n        \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nHimanshu Gupta\n\n\n    PhD Student\n    Himanshu.Gupta@colorado.edu\n\n\n\n    \n    \n        Detailed Bio\n        \n            Hey! I’m Himanshu. I am a Ph.D student in the Aerospace Department at CU Boulder. Currently, my research interests include - sequential decision making in partially observable environments that have arbitrary uncertainties, autonomous navigation for vehicles and manipulators among dynamic obstacles, and designing continuous action space online POMDP solvers. Prior to this, I finished my Masters degree in Computer Science from CU Boulder and my bachelor’s degree in Computer Science from Indian Institute of Technology (IIT) Ropar. In my free time I like to explore the beautiful city of Boulder and its neighboring towns on my bike, play basketball, cook delicious Indian curries, read mystery novels, or just watch animes and TV shows!\n\n        \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nQi Heng Ho\n\n\n    PhD Student\n    qi.ho@colorado.edu\n\n\n\n    \n    \n        Detailed Bio\n        \n            Hey there! I’m Qi Heng, a PhD student at CU Boulder. My research interests include sequential decision making under uncertainty, formal synthesis for robotics, and the intersection of these fields. I am especially interested in understanding how we can develop time and safety-critical autonomous systems that perform reliably with behavioral guarantees in partially observable and unstructured environments. Previously, I was a research engineer at the Singapore-MIT Alliance for Research and Technology Centre, where I worked on developing algorithms for self-driving vehicles. I grew up in Singapore, and completed my undergraduate degree at the National University of Singapore.\n\n        \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nTyler Becker\n\n\n    PhD Student\n    Tyler.Becker-1@colorado.edu\n\n\n\n    \n    \n        Detailed Bio\n        \n            I’m a PhD student at CU Boulder and received my bachelor’s degree in aerospace engineering at Rutgers University New Brunswick, New Jersey. My research interests lie at the intersection of decision making under uncertainty and game theory. I began my research by investigating POMDP solution methods for finding cost-effective coronavirus testing strategies. Since then, I’ve moved to applying counterfactual regret methods to custody maintenance of adversarial satellites in the space domain awareness field. I’m currently looking into bridging the gap between the deterministic/continuous strategies offered by solutions to differential games and the stochastic/sequential strategies offered by solutions to imperfect information extensive form games.\n\n        \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nZakariya Laouar\n\n\n    PhD Student\n    Zakariya.Laouar@colorado.edu\n\n\n\n    \n    \n        Detailed Bio\n        \n            Hi, I’m Zaki. I am a second-year Ph.D. student in the Department of Aerospace Engineering at the University of Colorado Boulder. I received a B.Sc. and M.Sc. in Aerospace Engineering also at CU Boulder. My current research revolves around planning and control under uncertainty. I have worked on contingency management of UAS under component failures using MPC and POMDP methods. I am currently working on adaptive and transparent human-in-the-loop planning for search and rescue. In my free time, I enjoy playing soccer and shredding the gnar.\n\n        \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nJackson Wagner\n\n\n    PhD Student\n    jackson.wagner-1@colorado.edu\n\n\n\n    \n    \n        Detailed Bio\n        \n            Hey! My name is Jackson and I’m a Master’s student in the Electrical Engineering department at CU Boulder studying dynamics and controls. I recieved my bachelor’s in Electrical engineering from Washington State University. My current research involves using reinforcement learning to find causes of failure in systems, a method called Adaptive Stress Testing. In my free time I enjoy playing games such as chess and Magic: The Gathering.\n\n        \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nLisa Ventura\n\n\n    PhD Student\n    lisa.ventura@colorado.edu\n\n\n\n    \n    \n        Detailed Bio\n        \n            Hi, I’m Lisa. I am a PhD student in the Aerospace Engineering Department at CU Boulder. Currently, my research interests include human-autonomy teaming, risk management in partially observable environments with uncertainty, autonomous navigation to minimize risk, and optimizing  human trust of collaborative autonomous systems. Prior to this, I earned a masters degree in Computer Science from Georgia Tech. I’m co-advised by Allie Hayman in the Bioastronautics focus area and work on trust modeling and prediction.\n\n        \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nMark Boyer\n\n\n    PhD Student\n    mark.boyer@colorado.edu\n\n\n\n    \n    \n        Detailed Bio\n        \n            Hi I’m Mark!  I’m currently a PhD student in aerospace engineering at CU Boulder.  I received my Bachelors in Aeronautical Engineering from the US Air Force Academy and my Master’s in the same from MIT.  My research will hopefully focus upon better understanding the critical attributes and consequences of explainability in human-autonomy teaming in high-criticality environments.  I am a test pilot in the Air Force with experience mostly in the F-35 and T-38, but have been lucky enough to fly over 20 different types of aircraft over 10 years of flying.  Outside of work, I love getting outdoors hiking, camping, mountain biking, skiing, or playing golf.\n\n        \n    \n\n\n\nMaster’s Students\n\nVisiting Scholars\n\n\n\n\n\n\n\n\n\n\n\nMel Krusniak\n\n\n    Visiting Scholar\n    mel.krusniak@vanderbilt.edu\n\n\n\n    \n    \n        Detailed Bio\n        \n            I’m a third-year PhD student at Vanderbilt University, working with ADCL over the summer of 2025 to develop a Julia framework for working with fully continuous, differentiable partially observable multiagent scenarios. At my home lab - the Vanderbilt Mathematical Programming and Intelligent Robotics Lab (VAMPIR) - I build algorithms for imperfect information games in robotics, targeting partially competitive, partially cooperative applications. In the past / on the side, I’ve worked with UAVs for conservation, computer vision in surgical robotics, and machine learning for road maintenance monitoring.\n\nI completed my Bachelor’s degree in Computer Science at Yale University in 2022. In my spare time, I enjoy board games and writing, and I’m an amateur sewist.\n\n        \n    \n\n\n\n\n\nAlumni\n\n\n\n\n\n\n\n\n\n\n\nJohn Tucker\n\n\nMS Student\n\n\n\n    Position after ADCL: PhD Student at Stanford University\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJuyeop Han\n\n\nVisiting Scholar\n\n\n\n    Position after ADCL: PhD Student at MIT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMichael H. Lim\n\n\nPhD Student\n\n\n\n    Position after ADCL: AI Scientist at C3.ai\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSujeong Park\n\n\nVisiting Scholar\n\n\n\n    Position after ADCL: PhD Student at KAIST\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWilliam Pope\n\n\nMS Student\n\n\n\n    Position after ADCL: United States Space Force Officer\n\n\n\n\n",
    "url": "/people/"
  },
  
  {
    "title": "Publication Timeline and Checklist",
    "excerpt": "\n",
    "content": "You can copy the markdown source for this page and edit it to keep track of your progress.\n\nPre-Submission Schedule\n\nThe schedule below is a target for the weeks leading up to a conference or journal with a deadline on D-day (i.e. D-14 indicates 14 days before the deadline). I expect this schedule to be comfortable for an advanced PhD student who already has the initial experiments completed.\nNew students may want to extend the target times because each of the tasks will take them longer.\n\nThis schedule may slip a bit, and this is normal. However, if the schedule slips by 50% (i.e. the D-28 tasks are not completed before D-14), we will call off the submission and wait for the next conference.\n\nD-28\n\n  Overleaf created and shared - include the venue in the overleaf project name.\n  All potential co-authors have confirmed that are aware of the paper and have access to overleaf.\n  The expected contributions have been outlined near the beginning of the paper.\n  If the review process is not double blind, all Acknowledgement to funding sources have been inserted.\n\n\nD-21\n\n  Initial figures and results tables are in the pdf.\n  Literature review is complete (all relevant papers are cited in the pdf, and a brief description of how they relate to the new work is outlined)\n  Section and paragraph-level outline for Dr. Sunberg to initially review. Notify Dr. Sunberg when this is ready and expect review to take 3 days.\n\n\nD-14\n\n  50% of final text complete\n  Figures are in their final form (minor updates to the data may come after this, but all of the axes, main trends, should stay fixed. The figures should not have any major problems that would prevent publication.)\n\n\nD-7\n\n  Complete draft ready for final review. Notify Dr. Sunberg when this is ready and expect review to take 3 days.\n\n\n(Optional/Conditional)\n\n\n  Add entry to the top of unsubmitted.bib\n    \n      The entry type should be @unsubmitted\n      In the note field, add the words “In preparation” and indicate the expected submission date and venue\n    \n  \n  Upload to arxiv. Check to see if the venue has any special policies about arxiv.\n\n\nAfter Submission\n\nBefore Acceptance\n\n\n  Add new entry to the top of our-pubs.bib:\n    \n      Add the extra field note={Under review}\n      Add a field annotating which authors are lab members (this helps for proposals). If the first and second authors are students and Zach is the fourth author, you would use author+an={1=student;2=student;4=zach}.\n      Remove old entry from unsubmitted.bib\n    \n  \n\n\nAfter Acceptance\n\n\n  Update entry in our-pubs.bib by removing the under/review\n  Add entry to news.csv\n  Make sure Acknowledgements to all funding sources have been included.\n\n\nAfter Final Camera-Ready Submission\n\n\n  Update arxiv to match submission and with proper links to the published article.\n\n",
    "url": "/posts/publication-checklist/"
  },
  
  {
    "title": "Publications",
    "excerpt": "\n",
    "content": "Publications\n\n\n\n",
    "url": "/publications/"
  },
  
  {
    "title": "Research",
    "excerpt": "\n",
    "content": "Our research focuses on artificial intelligence for controlling complex systems. We usually focus on autonomous aerospace systems, from space probes to drones to telescopes for tracking space debris, but we have also worked on other systems including ecological communities and autonomous cars. The theme that unites all of our research is uncertainty. This could be uncertainty in the system’s parameters or states or in how other people, systems, or the environment will interact with it. We approach our research from many perspectives, ranging from pure mathematical theory to numerical simulation to hardware experiments. The mathematical formalisms that we most often use are the partially observable Markov decision process (POMDP) for stochastic uncertainty, or various game formalisms when the uncertainty is worst case or introduced by other rational agents. One of our most important specialties is developing online tree search algorithms for POMDPs, and we are one of the world’s leading centers of development for this approach.\n\nThe best way to view our most up-to-date research is to look at our Publications page. Some slides and brief descriptions of some of our projects can be found below. (If you cannot see the slides, make sure to disable all add blockers.)\n\n\n\nApplications and Extensions\n\nSearch and Rescue\n\nAutonomous aircraft have the potential to vastly improve our ability to undertake remote sensing tasks such as search and rescue and wildfire monitoring. However, autonomous systems may not have the contextual knowledge that human expert operators have. We’re partnering with the COHRINT Lab to integrate our intelligent POMDP planners with input from an operator to improve search and rescue.\n\n\n\nEcological Navigation\n\n\nEcological management problems often involve navigating from an initial to a desired community state. We showed that navigation between states is an equivalent problem to searching for lowest-cost sequences of actions that comprise direct and shortcut paths. Shortcuts can be obtained by using small sequential abundance perturbations (e.g. low-density introductions) and environment perturbations to nudge communities between states. Our work suggests that brute-force approaches to navigation like antibiotics or clearcutting may have realistic and less impactful alternatives.\n\n\n  Navigation between initial and desired community states using shortcuts, Blonder et al. (2023), In Ecology Letters.\n\n\nBehavior-Aware Autonomous Driving\n\n\n\nIn autonomous driving, there is an inherent tradeoff between safety and efficiency, especially time-efficiency.\nIf a self-driving car is to be perfectly safe, it cannot enter the road, and it can be the fastest if there are no safety constraints.\nThis tradeoff results in the Pareto curves shown in the figure below.\nBut the performance also depends on the model.\nWe showed that by modeling the latent internal states of the other drivers on the road, safety and efficiency can both be simultaneously improved (this corresponds to moving the Pareto curve).\nIn computational tests in a highway driving scenario, internal state modeling allowed the autonomous vehicle to perform a multiple-lane change maneuver nearly twice as fast with the same level of safety.\n\n\n  \n  \n  \n  ![Highway Lane Change Planning](/assets/images/highway_planning.png)\n\n\n\n  Improving Automated Driving through POMDP Planning with Human Internal States, Sunberg and Kochenderfer (2022), In IEEE Transactions on Intelligent Transportation Systems.\n  The Value of Inferring the Internal State of Traffic Participants for Autonomous Freeway Driving, Sunberg et al. (2017), Pre-print.\n\n\nAutonomous Autorotation\n\nIn 2013, Professor Sunberg (as an MS student) and collaborators used autorotation to repeatedly successfully land a small autonomous helicopter without power.\nThe video below contains footage of one of the landings from a nose-mounted camera.\nNote that the pilot releases control as he turns off the motor.\n\n\n\n\n  A Real-Time Expert Control System For Helicopter Autorotation, Sunberg et al. (2015), In Journal of the American Helicopter Society.\n\n\nTheoretical Foundations\nOptimality of POMDP Approximations\n\n\nPartially observable Markov decision processes (POMDPs) provide a flexible representation for real-world decision and control problems. However, POMDPs are notoriously difficult to solve, especially when the state and observation spaces are continuous or hybrid, which is often the case for physical systems. We present a general theory characterizing the approximation error of the practically effective particle filtering techniques that many recent online sampling-based POMDP algorithms use.\n\n\n  Optimality Guarantees for Particle Belief Approximation of POMDPs, Lim et al. (2023), Journal of Artificial Intelligence Research (JAIR).\n\n\nPOMDP Algorithms with Theoretical Guarantees\n\n\n\nRecent online sampling-based algorithms that use techniques such as observation likelihood weighting and have shown unprecedented effectiveness in domains with continuous observation and action spaces. This line of work offers theoretical justifications of these techniques, proving that our new algorithms that utilize these techniques will estimate Q-values accurately with high probability and can be made to perform arbitrarily near the optimal solution by increasing computational power.\n\n\n  Voronoi Progressive Widening: Efficient Online Solvers for Continuous State, Action, and Observation POMDPs, Lim et al. (2021), In the proceedings of IEEE Conference on Decision and Control (CDC).\n  Sparse tree search optimality guarantees in POMDPs with continuous observation spaces, Lim et al. (2020), In the proceedings of International Joint Conferences on Artificial Intelligence (IJCAI).\n\n\nAlgorithmic Developments\n\nPractical POMDP algorithms\n\n\n\nLeading online partially observable Markov decision process (POMDP) solvers such as POMCP and DESPOT can handle continuous state spaces, but they still struggle with continuous action and observation spaces.\nIn fact, it can be shown analytically that they will converge to suboptimal solutions for some POMDPs with continuous action spaces regardless of the amount of computation.\nIn this line of work, we propose novel POMDP algorithms that overcome these problems using techniques such as progressive widening and weighted particle filtering.\n\n\n\nFor example, in the light-dark example above, POMCP (left) cannot decide to localize in the light region, while our new algorithm POMCPOW (right) can, allowing it to hit the target at the origin much more quickly.\n\n\n  Voronoi Progressive Widening: Efficient Online Solvers for Continuous State, Action, and Observation POMDPs, Lim et al. (2021), In the proceedings of IEEE Conference on Decision and Control (CDC).\n  Online algorithms for POMDPs with continuous state, action, and observation spaces, Sunberg and Kochenderfer (2018), In the proceedings of International Conference on Automated Planning and Scheduling (ICAPS).\n\n\n",
    "url": "/research/"
  },
  
  {
    "title": "Resources",
    "excerpt": "\n",
    "content": "This page contains a list of resources that we maintain for students in the ADCL and the broader community.\n\nOpen Source Software\n\n\n  POMDPs.jl An interface for defining, solving, and simulating fully and partially observable Markov decision processes on discrete and continuous spaces.\n\n\nCourse list\n\nWe maintain a list of courses related to AI and Robotics at CU Boulder: CU Boulder AI and Robotics Classes\n\nOther information, tips, and advice\n\n\n  Communicating at the Edge of Your Knowledge\n  Writing Tips\n  List of Conferences\n\n\nLab policies and procedures\n\n\n  Publication Timeline and Checklist\n  Paper Progress\n  Yearly Check-in Instructions\n  Lab Computing\n  Lab logos can be found in the logo github repo\n\n\nOld Items\n\n  Summer 2025\n  2024 Lab Meeting Schedule\n\n",
    "url": "/resources/"
  },
  
  {
    "title": "Bridging Language and Action: How Vision-Language-Action Models and Reinforcement Learning Enable Intelligent Robotic Decision Making",
    "excerpt": "\n",
    "content": "1. Introduction\n\nThe intersection of natural language understanding and robotic control is an exciting, active area of research in robotics. Advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) paved the way for Vision-Language-Action (VLA) models which are systems capable of translating high-level human instructions into executable robot behaviors. At the same time, Reinforcement Learning (RL) remains the dominant framework for solving sequential decision-making problems in robotics. The convergence of these two approaches offers a promising path toward building general-purpose robotic systems that can understand human intent, reason about their environment, and adapt to novel situations.\n\nWhen I first learned about VLAs, I imagined them as a substitute for traditional Reinforcement Learning. Consider a simple navigation task: moving a robot through a grid world to reach a goal state. In a classical RL setup, the robot would explore through trial and error, eventually learning an optimal policy from reward feedback. In contrast, I pictured a VLA-based system where I could simply instruct the robot, “Go to the green circle,” and it would infer the necessary sequence of actions from visual input. This framing makes RL and VLAs seem fundamentally distinct.\n\nHowever, recent research suggests otherwise. RL and VLAs are being used in combination, from the use of RL during pre-training and supervised fine-tuning to hierarchical control stacks in autonomous navigation. So this blog post that began as a comparison between two seemingly distinct paradigms became an exploration of their interconnected use cases. In this post, we’ll examine how RL and VLA models complement each other in addressing core challenges in robotics. We’ll introduce some theoretical foundations and then discuss practical integration strategies that enable robots to combine semantic understanding with low-level adaptive control.\n\n2. Foundations\n\nBefore diving into how Reinforcement Learning (RL) and Vision-Language-Action (VLA) models intersect, let’s briefly review their conceptual foundations.\n\n2.1 Reinforcement Learning\n\nReinforcement Learning is a framework for sequential decision-making under uncertainty. We’ll consider the fully observable case here, but know that there’s also a partial observability case where parts of the environment are hidden or noisy.\n\nAn RL agent interacts with an environment commonly modeled an Markov Decision Process (MDP), formalized as a tuple $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$ where:\n\n\n  $\\mathcal{S}$ is the state space representing possible robot and environment configurations\n  $\\mathcal{A}$ is the action space containing available robot actions\n  $P: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\to [0,1]$ is the transition probability function\n  $R: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$ is the reward function\n  $\\gamma \\in [0,1)$ is the discount factor\n\n\nThe Markov property ensures that the future state depends only on the current state and action:\n\n\n  \n    \n      [P(s_{t+1}\n      s_t, a_t, s_{t-1}, a_{t-1}, \\ldots) = P(s_{t+1}\n      s_t, a_t)]\n    \n  \n\n\nAt each timestep $t$, the agent observes a state $s_t$, takes an action $a_t$, receives a reward $r_t$, and transitions to a new state $s_{t+1}$. The goal is to learn a policy $\\pi(a|s)$ that maximizes the expected cumulative discounted reward:\n\n[J(\\pi) = \\mathbb{E}{\\tau \\sim \\pi} \\left[ \\sum{t=0}^{\\infty} \\gamma^t r_t \\right]]\n\nwhere $\\tau = (s_0, a_0, s_1, a_1, \\ldots)$ denotes a trajectory sampled by following policy $\\pi$.\n\nRL algorithms approach this optimization problem in different ways. Value-based methods learn to estimate how good it is to be in a state or take an action. The state-value function $V^\\pi(s)$ represents the expected return from state $s$ under policy $\\pi$:\n\n[V^\\pi(s) = \\mathbb{E}{\\tau \\sim \\pi} \\left[ \\sum{k=0}^{\\infty} \\gamma^k r_{t+k} \\mid s_t = s \\right]]\n\nSimilarly, the state-action value function (Q-function) quantifies the expected return from taking action $a$ in state $s$:\n\n[Q^\\pi(s, a) = \\mathbb{E}{\\tau \\sim \\pi} \\left[ \\sum{k=0}^{\\infty} \\gamma^k r_{t+k} \\mid s_t = s, a_t = a \\right]]\n\nPolicy gradient methods directly optimize the policy parameters $\\theta$ to maximize $J(\\pi_\\theta)$ by computing gradients:\n\n\n  \n    \n      [\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}{\\tau \\sim \\pi\\theta} \\left[ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t\n      s_t) \\cdot A^\\pi(s_t, a_t) \\right]]\n    \n  \n\n\nwhere $A^\\pi(s, a) = Q^\\pi(s, a) - V^\\pi(s)$ is the advantage function, measuring how much better action $a$ is compared to the average action in state $s$.\n\nModern deep RL algorithms leverage neural networks to approximate these functions in high-dimensional spaces. Proximal Policy Optimization (PPO) [2] is an on-policy algorithm that uses neural networks to represent both the policy $\\pi_\\theta(a|s)$ and value function $V_\\phi(s)$, updating them through clipped policy gradients to ensure stable learning. Soft Actor-Critic (SAC) [1] is an off-policy actor-critic method that simultaneously learns a policy $\\pi_\\theta$ and Q-function $Q_\\phi$ while maximizing both expected return and entropy. Group Relative Policy Optimization (GRPO) [3] is a policy gradient method that leverages group-based relative comparisons to improve learning efficiency and has been used for fine-tuning large pretrained models. These are just a few examples of RL algorithms. There are many more!\n\nMany of these RL algorithms have been successfully applied to robotic control because they can learn directly from high-dimensional inputs (like images) and handle continuous action spaces. However, RL can still suffer from sample inefficiency, reward engineering challenges, and limited generalization to novel tasks or new domains. It’s possible that VLA models may help address these issues.\n\n2.2 Vision-Language-Action Models\n\nVision-Language-Action (VLA) models emerge from the foundation model paradigm. These systems combine large-scale pretraining on multimodal datasets (images, text, and sometimes video or actions) to learn joint representations that connect visual perception, linguistic understanding, and physical interaction. These models leverage internet-scale data leading to pretrained models that have exposure to much more diverse data than a typical robotics dataset.\n\n\n  Vision encoders (e.g., ViTs, CNNs) map images or visual observations into latent embeddings.\n  Language encoders/decoders (e.g., Transformers, LLMs) process textual inputs or instructions.\n  Action modules map internal representations into motor commands, joint torques, or discrete control primitives.\n\n\nIn a VLA, these components are often connected through a shared embedding space or a transformer-based architecture that fuses multimodal information. This enables the system to interpret instructions such as “Pick up the red cube and place it on the blue block” and produce a coherent sequence of actions. There are many different action token representations, but for the sake of this post just envision directly outputting robotics controls. For an example, see Figure 1 which shows how image and text are input into the VLA which then outputs a vector of robot controls for a gripper.\n\n\n  \n  Figure 1: OpenVLA architecture showing the integration of vision encoders, language models, and action prediction modules. Diagram from Liu et al. [11], representing the OpenVLA model [12].\n\n\n2.3 Conceptual Contrast\n\n\n  \n    \n      Aspect\n      Reinforcement Learning\n      Vision-Language(-Action) Models\n    \n  \n  \n    \n      Core Objective\n      Maximize cumulative reward via interaction\n      Learn multimodal representations and semantic grounding\n    \n    \n      Learning Signal\n      Scalar rewards from environment\n      Supervised or self-supervised cross-modal alignment\n    \n    \n      Data Source\n      Experience (simulated or real)\n      Large curated datasets (image–text–action triples)\n    \n    \n      Strengths\n      Adaptive control, exploration, online learning\n      Generalization, compositional reasoning, instruction following\n    \n    \n      Limitations\n      Sample inefficiency, narrow task focus\n      Lack of grounding without interaction, weak low-level control\n    \n  \n\n\n2.4 Toward Integration\n\nWhile these approaches originated separately, current research explores how to combine them for enhancing robot capabilities. RL provides a mechanism for adaptive control and feedback-driven learning, while VLAs supply semantic priors and contextual understanding. It’s hypothesized that integrating the two will enable robots to act optimally and also understand what they are doing and why.\n\n3. Where RL Meets VLA\n\nThe combination of Reinforcement Learning with Vision-Language-Action models is an active area of research and several promising strategies have emerged. In this section, we’ll explore two main approaches: fine-tuning VLAs with RL and hierarchical architectures that combine both.\n\n3.1 RL Fine-Tuning of VLA Models\n\nVLA models have shown promise generalizing to new situations, but they can fall short when tasks demand high precision think contact-rich manipulation or tasks where exact positioning matters like low-level joint control. This is where RL fine-tuning comes in, allowing us to directly optimize the VLA policy using task-specific rewards.\n\nSeveral recent papers have shown different ways to fine-tune VLAs with RL:\n\nVLA-R1 [4] integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) in an effort to provide VLAs with chain-of-thought style reasoning capabilities seen in recent LLMs. This approach helps VLA models better reason about object affordances and generate action sequences that are physically plausible not just semantically correct.\n\niRe-VLA [6] tackles one of the practical challenges: direct RL fine-tuning can be computationally expensive and unstable. Their solution is an iterative framework that alternates between RL updates and supervised learning, in an effort to get the benefits of both approaches.\n\nThe main challenge here is a balancing performance improvements on specific tasks without losing the broad generalization that makes VLAs useful in the first place. Too much fine-tuning and the model risks catastrophic forgetting. Not enough fine-tuning and the model will be unable to perform well on the specified robotics specific tasks.\n\n3.2 Hierarchical Architectures\n\nAnother strategy uses hierarchical architectures where VLA models and RL work at different levels of abstraction.\n\nOne approach is to separate high-level planning and low-level control:\n\n\n  High-level (VLA): Interprets language instructions and outputs subgoals or high-level action choices\n  Low-level (RL): Executes those high-level commands and handles the nitty-gritty details of motor/actuator/joint control\n\n\nThis division of labor has a practical advantage: the VLA can run at a slower rate while the RL controller runs fast. This matters because current VLAs are still slower than traditional low-level controllers. You don’t want your robot waiting around for the VLA because it could cause instability in the controls.\n\nNaVILA [8] is a great example of this approach in action (pun intended). The VLA gets fine-tuned to output “mid-level actions” (e.g. move forward 75 centimeters) which then feed into a PPO-trained RL policy. The RL policy takes those mid-level commands and figures out the specific joint movements needed to execute them. The researchers demonstrated this on real legged robots navigating different environments based on language commands. See the diagram below that shows the NaVILA system [8].\n\n\n  \n  Figure 2: NaVILA hierarchical architecture. The VLA generates mid-level actions that are executed by a low-level RL policy trained with PPO. From Cheng et al. [8].\n\n\nIRL-VLA [9] applies a similar hierarchical idea to autonomous driving, using a three-stage approach:\n\n  Pretrain a VLA policy through imitation learning\n  Build a reward world model using inverse RL\n  Use that reward model to guide further RL training (with PPO)\n\n\nThe innovation proposed in this paper is that the reward model lets you train VLA agents with reinforcement learning without having to rely on a simulator.\n\n4. Applications and Future Directions\n\nThe integration of VLA models and RL has enabled capabilities in several robotics domains (i.e. navigation, manipulation). VLA models can give robots the language and perception to understand our goals, while RL can give them the experience and feedback to achieve those goals effectively. As these two approaches continue to merge, we move closer to robots that can learn new tasks from natural instructions and improve through experience, just like humans do.\n\nQuestions to ponder (potential future research directions):\n\n\n  \n    Can we use VLAs to design methods that let RL fine-tune robot behavior with fewer real-world trials and less data?\n  \n  \n    Can a robot trained in simulation use the semantic understanding from a VLA to adapt more smoothly to the real world?\n  \n  \n    What would it take for a robot to know when it’s unsure about its perception or decision, and explore safely as a result? Can a combination of VLAs and RL lead to verifiably safer systems?\n  \n  \n    Could natural language become a way for people to give feedback and guide a robot’s learning process in real time? How would this combination of natural language through human-feedback differ from a typical Reinforcement Learning through Human Feedback paradigm (RLHF)?\n  \n\n\n5. Conclusion\n\nIntegrating Vision-Language-Action models and Reinforcement Learning is a promising direction for improving robot capabilities. VLA models provide semantic understanding, broad generalization, and efficient learning from diverse offline data. RL contributes adaptive optimization, fine grained control, and the ability to discover novel behaviors through environmental interaction.\n\nBy carefully integrating these approaches whether through direct RL fine-tuning or hierarchical architectures, we can build robotic systems that combine the semantic richness of large-scale pre-training with RL’s ability to produce precise low-level control. As these methods mature and scale, we move closer to more capable robots that can understand natural language instructions, reason about their environment through visual perception, and continuously improve their capabilities through experience.\n\n6. References\n\nNOTE: Whenever possible, this post references peer-reviewed literature from the robotics domain. However, some of the most recent works are still in review and thus have not been through the peer-review process yet. These cited works are preprint editions and their Arxiv links are provided.\n\n\n  \n    Haarnoja, T., Zhou, A., Abbeel, P., &amp; Levine, S. (2018). Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. ICML 2018.\n  \n  \n    Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). Proximal Policy Optimization Algorithms. arXiv preprint.\n  \n  \n    Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y.K., Wu, Y., &amp; Guo, D. (2024). DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv preprint.\n  \n  \n    Ye, A., Zhang, Z., Wang, B., et al. (2025). VLA-R1: Enhancing Reasoning in Vision-Language-Action Models. arXiv preprint.\n  \n  \n    Song, Z., Ouyang, G., Li, M., et al. (2025). ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models. arXiv preprint.\n  \n  \n    Chen, Y., et al. (2024). Improving Vision-Language-Action Model with Online Reinforcement Learning (iRe-VLA). ICRA 2025.\n  \n  \n    Wang, Y., Sun, Z., Zhang, J., et al. (2024). RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback. ICML 2024.\n  \n  \n    Cheng, A., et al. (2025). NaVILA: Legged Robot Vision-Language-Action Model for Navigation. Robotics: Science and Systems 2025.\n  \n  \n    Jiang, A., Gao, Y., Wang, Y., et al. (2025). IRL-VLA: Training a Vision-Language-Action Policy via Reward World Model. arXiv preprint.\n  \n  \n    Liu, J., Gao, F., Wei, B., Chen, X., Liao, Q., Wu, Y., Yu, C., &amp; Wang, Y. (2025). What Can RL Bring to VLA Generalization? An Empirical Study. arXiv preprint.\n  \n  \n    Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E., Lam, G., Sanketi, P., et al. (2024). OpenVLA: An Open-Source Vision-Language-Action Model. arXiv preprint.\n  \n\n",
    "url": "/posts/dmupp-2025/rl-vla-robotics/"
  },
  
  {
    "title": "The Future of Safe RLHF (Reinforcement Learning from Human Feedback)",
    "excerpt": "\n",
    "content": "Suppose You Want Text Generation — Why Not Just Normal RL?\n\n\n\nIf you just want an LLM to finish your sentences (and sandwiches), then look no further than traditional reinforcement learning (RL).\n\nBut if you want a more useful answer, normal supervised learning techniques fall short because:\n\n  Humans have to write high-quality examples from scratch for training data (expensive, time-consuming)\n  Misalignment exists between the training objective (predict the next token) and what is actually useful to a human (an answer to a question). That is, the model is trained to produce similar outputs, but we want useful outputs, which is not necessarily the same thing.\n\n\nRLHF rides in like a hero to solve this gap:\n\n\nBecause collecting preference data is cheaper than manual data creation, less labor is required to improve the model. Once the reward model learns patterns in human preferences, future outputs are more aligned with what users actually want in response to their queries. The LLM is fine-tuned via PPO to ultimately produce these outputs that humans prefer.\n\n\n\nThe three sentence overview of simple RLHF (starting in the bottom middle block in the flow diagram above): we feed prompts to an LLM, whose responses are sent to human annotators, who then compare pairs of these outputs and indicate a preference between them. The comparisons are converted to scalar scores via principles from the Elo algorithm, and the reward model learns from these preferences to maximize their score. This reward model is then used to train the LLM via PPO, and ultimately an aligned model emerges from this training.\n\n\nFrom a Meta paper on their Llama2 model, where RLHF methods exceed Supervised Fine-Tuning (SFT) in reward model score. Many researchers have demonstrated the benefits of RLHF, and furthermore, prominent LLMs like ChatGPT and Gemini use RLHF to fine-tune their own models. It has become a ubiquitous tool in modern RL.\n\n\n\nNow suppose you were an RLHF annotator. Would you prefer a response that is helpful-but-dangerous or safe-but-useless?\n\n\nWhy not both?\n\nSome researchers contend that this question is a false binary. We want to maximize usefulness and minimize harm simultaneously. Enter Safe RLHF.\n\nSince traditional RLHF optimizes a single scalar reward, it is poorly suited to pursuing both of these objectives. Current vanilla RLHF requires manual tuning to balance between helpfulness and harmlessness (Touvron 2023), and it risks finding local optima when optimizing solely for one preference (Casper 2023).\n\nThe Core Idea of Safe RLHF\n\nDecoupling Preferences\n\nTraditional RLHF: One reward scalar, one signal\n\nSafe RLHF: Two separate signals:\n\n  Helpfulness → Reward model\n  Harmlessness → Cost model\n\n\nEach model is trained on separate human-labeled pairwise preferences for helpfulness and harmfulness.\n\nThe objective of Safe RLHF becomes\n\n\nWe are maximizing the reward function such that the cost function is less than or equal to 0. D is a distribution of prompts used in the RL phase, and y are the responses generated by the LLM. The variable d is a hyper-parameter used to exert control of the probability of generating harmful responses. R and C are reward and cost respectively.\n\nThe Lagrangian formulation is then\n\n\nThe reward model for Safe RLHF is\n\nwhere the w and l subscripts are for win and loss respectively, based on the preferences of the annotator. This reward model follows from the Bradley-Terry model and maximum likelihood estimation. The cost model is similar, except they include the addition of extra weights based on if response y is harmful or harmless:\n\n\n\nIn this formulation, they use Lagrangian multipliers to penalize costs that exceed the constraint (a harmfulness threshold), and by separating the reward and cost models, they achieve multi-dimensional optimization (maximizing helpfulness while ensuring harmfulness is less than or equal to 0).\n\nHere is the overall schema of Safe RLHF:\n\n\nThe SFT model on the left was taken off the shelf, and they inputted prompts to that model to get the responses A and B shown in the ‘Data Collection’ section. These responses were then fed to the human annotators.The bottom-right section (labeled ‘RLHF’) depicts the process of normal RLHF. The Safe RLHF contribution is in the top right corner, where a reward and cost model are trained simultaneously.\n\nData collection\n\nAfter the model outputted responses based on the researchers’ chosen prompts, crowdworkers annotated safety metalabels for each prompt-response pair. The crowdworkers analyzed the responses for 14 predefined categories of potential harm:\n\n  Hate Speech, Offensive Language\n  Discrimination, Stereotype, Injustice\n  Violence, Aiding and Abetting, Incitement\n  Financial Crime, Property Crime, Theft\n  Privacy Violation\n  Drug Abuse, Weapons, Banned Substance\n  Non-Violent Unethical Behavior\n  Sexually Explicit, Adult Content\n  Controversial Topics, Politics\n  Misinformation Regarding Ethics, Laws, and Safety\n  Terrorism, Organized Crime\n  Self-Harm\n  Animal Abuse\n  Child Abuse\n\n\nResponses were deemed safe if they did not contain any of the categories above. Given two responses of the same prompt, crowdworkers ranked the harmlessness and helpfulness independently. Helpfulness was judged on clarity, relevance, and quality. (These laborers were vetted using a test and a minimize score of 90%. Out of a pool of 200 applicants, they retained 70 annotators. The company that provided these workers provided a quality control review, and the researchers themselves audited 10% of the data from each batch.)\n\nThey performed three rounds of Safe RLHF to refine the model multiple times. In rounds 2 and 3, red teams found prompts that generated unsafe responses from the SFT model, and these dangerous responses were then given to the annotators as well.\n\nNumerical Results\nThe left-most plot shows the distribution of reward and cost on test data in the initial Safe RLHF iteration. Blue dots are responses that crowdworkers labeled as safe, and red dots contain one of the 14 harms listed above. These data are then used to train a reward model, and the middle plot depicts the distribution on the test set from the trained reward model. Interestingly but unsurprisingly, the unsafe answers had slightly higher rewards (were more useful). The plot on the right is from the trained cost model and demonstrates the higher cost of unsafe responses.\n\n\nFor evaluation, researchers compared models against each other. They used both humans and GPT-4 for these comparisons (GPT-4 can replace human evaluators in assessing alignment capabilities of LLMs, Chiang &amp; Lee 2023). In the figure below, Alpaca-7B was the base SFT model, and Beaver-v1 is the first round of their Safe RLHF algorithm. Beaver-v2 is the second round, and similarly Beaver-v3 is the third. Beaver-v3 clearly outperforms the base model and previous iterations, showing a continued improvement. In the bar graph at the right, the probability of harmful responses decreased from 53% in iteration 1 to 2.5% in iteration 3.\n\n\n\nFinally, the plot below compares these Safe RLHF iterations to plain PPO and a cost-model (CM) classifier. The Beaver versions had a higher win rate of harmlessness, accompanied by only a small increase in the win rate for helpfulness over these two other algorithms.\n\n\n\nStrengths and Main Paper Contributions\n\n\n  First paradigm to combine Safe RL with RLHF\n  Decouples human preferences during annotation (usefulness and harmlessness as separate questions)\n  Establishes two separate objectives:\n    \n      Reward = helpfulness\n      Cost = harmfulness\n    \n  \n  Trains separate models for each\n\n\nThis paper’s biggest win was demonstrating that by decoupling harmlessness and helpfulness, the responses of their model achieved safer outputs than traditional RLHF methods. Although the helpfulness rating was only nominally higher in Safe RLHF, the harmlessness was greatly improved. The researchers achieved the first full integration of Safe RL into RLHF, provided open source code and dataset for future community research, and ethically sourced their human labor.\n\nChallenges/Future Work\n\nThe research presents a few challenges. The cost of manual labor remains high (both monetarily and time-wise), and these processes are not very scalable. Humans exhibit bias, are expensive and slow, and have inconsistencies in their judgements. Beyond internal human flaws, the conflicts among humans remain a source of problems. When two humans don’t agree on a preference, which should the model listen to? Do they cancel each other out? How can a model train on conflicting data? Do these annotators need to agree on a definition of ‘harmless’ beforehand? These implementation details test the limits of algorithms that rely on human input for training data. We will address this in a section below.\n\nAnother interesting direction is removing static inputs and developing context-aware constraints. λ could adapt to specific topics, users, or domains.\n\nFinally, a problem with RLHF in general is the propensity to collapse to similar answers, also called mode collapse. Researchers developed a method called Verbalized Sampling (VS) that attempts to circumvent this phenomenon. They assert that “post-training alignment often reduces LLM diversity” because humans tend to prefer the same familiar text, and they prove that this ‘typicality bias’ is a root cause. They further show that VS increases diversity of responses by 1.6-2.1x over direct prompting by changing a prompt from “write me a song” to “generate 5 responses with their corresponding probabilities to this prompt: write me a song.” Because they request a distribution of responses, the outputs of the LLM retain more diversity, which increases their overall performance without negatively affecting accuracy or safety. The original paper is here.\n\nRLHF and Safe RLHF Spin-offs\n\nLlama2: PPO + Rejection Sampling\n\nMeta’s model Llama2 uses PPO as in Safe RLHF, but they add rejection sampling, which reinforces the reward mechanism and increases the model’s performance. Instead of giving all LLM responses to the human annotators, they sample k outputs, score them based on the best reward model accessible at the time of the experiment, and select only the best answer for a given prompt. This could reduce human labor, as annotators don’t have to waste their time on LLM responses that the model knows are of lower quality already.\n\n\n\nIn the above figure, taken from Touvron et al 2023, they show that through rejection sampling, the median of the rewards and the maximum of the rewards continue to diverge, as the algorithm accumulated more of the best responses and rejected the lower performing ones.\n\nPairwise PPO (P3O)\n\nThe authors describe the instability of PPO, especially when optimizing rewards trained with the Bradley-Terry Loss comparative reward model (which is what Safe RLHF uses). They state that the Bradley-Terry model is invariant to constant shift, while PPO is not, so that when two reward models contain identical information about human preferences, optimizing them via PPO can lead to different outcomes. Instead, they use log-probability difference instead of a scalar reward, which allows for a “perfect” (their word; risky to include in a paper) alignment between their algorithm and the comparative nature of the reward model. They avoid complications like estimating the value function and other normalization techniques, and they claim their algorithm outperforms PPO in win-rate percentage. Ultimately, they acknowledge that DPO (discussed below) marginally surpasses P3O in reward, but DPO has a considerably higher KL-divergence, which may be detrimental to the generation quality as it can diverge farther from previously learned preferences.\n\nRL from AI Feedback (RLAIF)\n\nResearchers also investigated the viability of having AI provide the output preferences instead of humans, and they found that it produced a similar success rate. As seen in the plot below, both RLHF and RLAIF were preferred at roughly the same rate above normal SFT. Additionally, RLAIF was more harmless than both plain SFT and RLHF. The authors claim that “RLAIF is equally preferred to RLHF” when they compared against each other directly, but they don’t provide a plot of this. Their sample size of this direct evaluation was only 70 examples that humans blindly ranked, and no statistically significant difference existed. They admit that “more systematic analysis is required to identify if these patterns exist at scale.”\n\n\n\nDirect Preference Optimization (DPO)\n\nDPO demonstrates that no PPO-based RL loop is needed after the annotators offer their preferences. As the graphic below depicts, DPO eliminates the sampling and hyperparameter tuning implicit to PPO. The authors show that DPO fine-tunes LLMs to align with human preferences at least as well than other methods, and in particularly DPO exceeds PPO-based RLHF. They identify a mapping between language model policies and reward functions that enables training an LLM to satisfy preferences directly (using cross-entropy loss) without RL. By deriving a closed-form loss equation to match preference distributions, the algorithm is able to skip the additional RL step. Ultimately, because DPO has both similar performance and a simpler implementation, DPO lowers the cost to training LLMs on preferences (in this case, still human preferences).\n\n\n\n\n\nShould we let machines dictate human preferences?\n\nIn the case of LLMs, humans design these tools to help other humans. The most efficient way to improve these RLHF systems is to minimize human labor, replacing people with algorithms capable of making similarly-outcomed decisions. Many voices in the human-centered AI field postulate that the most favorable outcomes consist of mostly human-in-the-loop systems. While Gen AI can augment human labor and automate drudgery, the most powerful advances will come in tandem with the guidance and wisdom of humans. The tension between involving human expertise, not relying on human labor, and wanting to accelerate RLHF progress with machine preferences drives the question: how much human input is required to still be considered human-in-the-loop? How much H needs to be in RLHF? Is there a certain percentage of algorithmically-derived preferences that must be audited by a human, or a certain agreement rate?\n\nAnother consideration beyond basic system-type labeling is whether the average user wants their Gen AI experience to be finetuned according to the preferences of a robot. Even if an algorithm is aligned to previous RLHF preferences, the extra layer of abstraction may reduce overall trust. Certainly, varying levels of risk exist for employment of automated data labeling. Consider the use case where a retail company deploys a specifically-tuned model for their website’s chatbot. If the chatbot was trained with computer-generated annotations in an RLHF scheme, likely no one will be harmed when the chatbot recommends that a consumer buy a more floral blouse that is on sale. If, on the other hand, the DMV replaces their employees with kiosks to reduce customer wait time, overzealous teens may be able to procure a driver’s license illegally and imperil Boulder’s backstreets (among other, more serious risks). The implicated question becomes: what is the threshold for consumers to use and trust a system that they know has been finetuned according to the preferences of a machine? How much risk are people willing to take with Gen AI systems that have been annotated only via algorithms? Roughly 47% of Americans use LLMs, and most use it as a black box, unfamiliar with the underlying methods of how it was trained. It therefore seems likely that many users will the accept increasingly non-human preferences for RLHF finetuning for low-stakes Gen AI uses cases, even if they know that no humans were used in the making of it, since they already (mostly blindly) accept RLHF outputs today. Further research is required to understand the limits of this trust beyond everyday ChatGPT/Claude/Gemini use.\n\nHow do we get RLHF to reflect changing societal values?\n\nIn the US, for example, most people were against legalizing gay marriage in 2009, but by 2011, most people supported it.\n\n\n\nThis distinct shift in opinion should be reflected in our Gen AI interactions as well. Few would condone a chatbot today who espouses homophobic content. As a society’s values change, so should the RLHF annotations. Updating these too frequently would be costly and superfluous, while clearly waiting too long causes misaligned Gen AI, but what is the best balance between the two extremes? How do scientists know when they need to execute another RLHF cycle? To what extent should the government be involved in regulating this frequency? Would market forces compel the private sector to update their models, even when it costs them additional overhead? Potentially yes, as we saw with the Grok incident, which sparked public outrage and statements from xAI representatives who vowed to remove anti-Semitic content. Does this necessarily portend a whack-a-mole policy, where models are only finetuned after the public uncovers another harmful response?\n\nA larger challenges becomes: how do we model our values as a society? Is it possible to input enough specifics (or are there too many to list?) to fully define the value space, or for generalizations like “do no harm” to fully capture the contours of our beliefs? Right now, the power of Safe RLHF is the number of individual humans that offer inputs that (ideally) represent the population. If the goal is to reduce this human labor, perhaps everyone needs an individual ‘proxy’ LLM that contributes their precise opinions, instead of a single algorithm that is annotating these preferences for an entire batch.\n\nConclusion\nMany exciting directions exist for Safe RLHF methods that increase performance, lower training costs, and more fully represent the desires of the humans who use these systems. By ensuring alignment to our values, we can ensure an experience for everyone that is both helpful and harmless.\n\nReferences\n\n\n  Humans in the Loop: The Design of Interactive AI Systems – Stanford HAI\n  Grok, Elon Musk’s AI chatbot, started calling itself ‘MechaHitler’ – NPR\n  U.S. Support for Gay Marriage Reaches New High – Statista\n  Estimating the Usage and Utility of LLMs in the U.S. General Public – Rethink Priorities\n  The Story of RLHF: Origins &amp; Motivations – Cameron R. Wolfe (Substack)\n  Llama2: Open Foundation and Fine-Tuned Chat Models\n  Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback\n  LLaMA: Open and Efficient Foundation Language Models\n  Can Large Language Models Be an Alternative to Human Evaluations?\n  Researchers find adding this one simple sentence to prompts makes AI models more reliable – VentureBeat\n  Llama 2: Open Foundation and Fine-Tuned Chat Models\n  Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment\n  RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback\n  Direct Preference Optimization: Your Language Model is Secretly a Reward Model\n\n",
    "url": "/posts/dmupp-2025/safe-rlhf/"
  },
  
  {
    "title": "Writing Tips",
    "excerpt": "\n",
    "content": "Passive voice and using “we”\n\nIn general, active voice (e.g. “He moved the box.”) is a more direct and easy-to-understand way of writing compared to passive voice (e.g. “The box was moved.”). However, many scientists and engineers use passive voice in their papers. A large portion of scientific papers is devoted to describing what experiments the scientists conducted. Often, the most natural subject of sentences describing these experiments is “we”, referring to the scientists who conducted the experiment and are writing the paper. However, scientists usually want to put the focus on the experiments rather than the experimenters. Thus, many scientists and engineers have adopted a rule that does not allow the use of “we” or “our” in technical writing. Focusing on the science is a good impulse, but unfortunately the easiest way to avoid “we” is to resort to using the passive voice, which makes papers more difficult to understand.\n\nThere is an additional confounding twist in this story: Mathematicians often use “we” in their papers, however they are typically referring to a different group of people than scientists and engineers. Usually, mathematicians seek to lead a reader through a line of reasoning, for example “Since 51 is a product of 3 and 17, we can conclude that it is not prime.” In this case, “we” refers to the author and the reader rather than the people who conducted an experiment. When used in this way “we” has been much more commonly accepted in mathematical writing than the “we” that refers to the authors in scientific writing.\n\nSo, we are left with the question: should we use “we” in sentences describing the experiments that we carry out?\nMy guidance is the following: It is acceptable to use “we” to avoid passive voice. However, whenever practical, writing should focus on the scientific concepts rather than the people writing the paper or doing the work. Here is an example:\n\n\n  Discouraged (passive): “A hyperparameter search was used to determine that 0.001 is the best learning rate.”\n  Discouraged (“We”): “We conducted a hyperparameter search to find that 0.001 is the best learning rate.”\n  Preferred: “A hyperparameter search revealed that the best learning rate is 0.001.”\n\n\n“We” generally does not need to be used in sections such as the background or problem formulation, for example\n\n\n  Discouraged: “We formulated the following POMDP model of the problem:”\n  Preferred: “The following POMDP models the important features of the problem:”\n\n",
    "url": "/posts/writing/"
  },
  
  {
    "title": "Yearly Check-in Preparation",
    "excerpt": "\n",
    "content": "The yearly check in is a time for us to review, reflect on, and communicate about your progress through the PhD.\n\nSlide Deck\n\nIn preparation, please reflect on the last year and future and prepare a slide deck or document using the outline below for inspiration (unless the bullets are marked as required, you do not need to respond to all of them). Please send it to me at least 48 hours before our meeting so that I can review it.\n\nNote that you can just paste this into slides.com here and it will automagically generate the slides so that you can fill them in.\n\n## Accomplishments in the last year\n- Do you feel you made good progress in your PhD program this year?\n- Summarize the following accomplishments\n  - Research discoveries/conclusions\n  - Papers submitted and published\n  - Conferences and workshops attended\n  - Courses taken\n  - Internships\n  - TA/TF positions\n  - Other activities such as mentoring, reviewing, and outreach\n\n---\n\n## Challenges/disappointments in the last year\n- Any challenges that have made it difficult to progress in your PhD that you would like to discuss with me\n- Any aspects of your PhD that you feel negative about and would like to discuss with me\n\n---\n\n## Planning for the next year\n- What projects do you expect to be working on in the Spring, Summer, and Fall?\n- What publications do you expect to submit and when?\n- Do you expect to take on any other significant responsibilities? (Make sure to indicate the relative priorities of this and the items above)\n- How do you plan to address any challenges or disappointments you faced in the past year?\n- If you could work on any project at all, what would it be like? (is it hardware/computational/theoretical, etc.) (note: it is probably not possible to do this, but I want to optimize as much as possible)\n\n---\n\n## Future\n\n- What do you plan to do after your PhD?\n\n---\n\n## Timeline and Requirements\n\n**REQUIRED** (These are questions on the department's evaluation form that I will need to fill out and you will need to sign)\n\n- \"Please list all uncompleted required coursework and the expected timeline for completion\"\n- \"Please provide a detailed plan outlining the expectations for completing the Preliminary Exam\" [Preliminary Exam](https://www.colorado.edu/aerospace/academics/graduates/phd-advising#expand-98405)\n- \"Please provide a detailed plan outlining the expectations and timeline for completing Comprehensive Exam\" [Comprehensive Exam](https://www.colorado.edu/aerospace/academics/graduates/phd-advising#expand-61455)\n- \"Please provide a detailed plan outlining the expectations and timeline for completing the Final\nExam\" [Defense](https://www.colorado.edu/aerospace/academics/graduates/phd-advising#expand-58994)\n\n---\n\n## Advising relationship\n- In what ways are our interactions most/least helpful?\n- Would you like me to do anything differently going forward?\n- Are there specific ways that I can help you push youself to achieve our goals next year?\n\n---\n\n## Anything else that you'd like to talk about\n\n",
    "url": "/posts/yearly-checkin/"
  }
  
]

